{
  "hash": "191aa5b3ef3318d6bf1089a0402da806",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Acoustic Emotion Classification\"\nsubtitle: \"Optimizing Performance Through Ensemble Methods\"\nauthor: \n  - name: \"Ralph Andrade\"\n    affiliations:\n      - name: \"College of Information Science, University of Arizona\"\ndescription: \"An emotion classification project using machine learning to identify four emotions (happy, sad, angry, fear) from speech audio recordings, comparing different algorithms to determine the most effective approach for acoustic emotion recognition.\"\nformat:\n  html:\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: true\n    embed-resources: true\neditor: visual\ncode-annotations: hover\nexecute:\n  warning: false\njupyter: python3\n---\n\n## Project Overview\n\nThis project investigates the application of machine learning techniques to classify emotions from speech audio using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset). The study focuses on distinguishing four primary emotions—happy, sad, angry, and fear—through acoustic feature analysis and explores how emotional intensity affects classification performance. By combining traditional feature engineering with ensemble learning methods, this research aims to develop robust emotion recognition models while examining the relationship between speech intensity and model confidence.\n\nThe project addresses two fundamental questions in acoustic emotion recognition: first, whether traditional machine learning algorithms can accurately classify emotions using engineered audio features, and second, how ensemble methods can significantly improve classification performance compared to individual models. This investigation contributes to the growing field of effective computing while demonstrating practical applications of ensemble learning principles in speech emotion recognition.\n\n## Dataset Description\n\n**Primary Dataset: CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset)** **Provenance:** CREMA-D is a validated multi modal database created through collaboration between researchers and crowd sourced validation. The dataset contains emotional speech recordings from professional actors, making it ideal for supervised learning approaches to emotion classification.\n\n### Dimensions & Structure:\n\n| **Attribute** | **Details** |\n|------------------------|------------------------------------------------|\n| **Total Files** | 7,442 audio clips from diverse emotional expressions |\n| **Speakers** | 91 professional actors (48 male, 43 female) |\n| **Age Range** | 20–74 years, providing demographic diversity |\n| **Target Emotions** | 4 emotions selected for analysis (happy, sad, angry, fear) |\n| **Emotional Intensities** | Multiple intensity levels (low, medium, high, unspecified) |\n| **File Format** | WAV files suitable for feature extraction |\n| **Sentence Variety** | 12 different sentences to reduce linguistic bias |\n\n**Dataset Selection Rationale:** CREMA-D was chosen for its substantial size, demographic diversity, and established use in emotion recognition research. The dataset's systematic organization and intensity labels directly support both research questions, while its availability through Kaggle ensures reproducible research practices.\n\n## Signal Processing (Extract, Transform, Load)\n\nThis pipeline enables the conversion of complex, high-dimensional waveforms into compact, informative representations—such as MFCCs or spectral centroids—that capture the essence of the sound.\n\n::: {#process-audio-signal .cell message='false' execution_count=1}\n``` {.python .cell-code}\n# Import libs\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport os\n\n# CREMA-D filename structure: ActorID_SentenceID_EmotionID_IntensityID.wav\naudio_dir = \"./data/CREMA_D/\"\n\ndef parse_cremad_filename(file_path):\n  name = os.path.splitext(os.path.basename(file_path))[0]\n  parts = name.split('_')\n  actor_id = parts[0]\n  sentence_id = parts[1]\n  emotion = parts[2]\n  intensity = parts[3].split('.')[0]\n  \n  # mapping emotion_id to emotions\n  emotion_map = {\"ANG\":\"angry\"\n  ,\"DIS\":\"disgust\"\n  ,\"FEA\":\"fear\"\n  ,\"HAP\":\"happy\"\n  ,\"NEU\":\"neutral\"\n  ,\"SAD\":\"sad\"}\n  \n  return {\n    \"actor_id\": actor_id\n    ,\"sentence\": sentence_id\n    ,\"emotion\": emotion_map[emotion]\n    ,\"intensity\": intensity\n  }\n\ndef extract_features(file_path):\n  \"\"\"\n    Iteratively extracts audio features from a given .wav file using librosa.\n\n    Features:\n        - Zero Crossing Rate\n        - Chroma STFT\n        - MFCCs\n        - Root Mean Square Energy\n        - Spectral Centroid\n\n    Returns:\n        list of dict: Each dictionary containing the features, and metadata parsed from the filename.\n  \"\"\"\n  # initalize empty list to store all features\n  all_features = []\n  \n  # iteratively scan os directory\n  for entry in os.scandir(file_path):\n    if entry.is_file():\n        # load .wav files iteratively from entry path\n        y, sr = librosa.load(entry.path, sr=22050)\n        \n        # initiallize empty dict\n        features = {}\n        \n        # 0. File Metadata\n        fmd = parse_cremad_filename(entry.name)\n        audio_duration = len(y)/ sr\n        features['actor_id'] = fmd['actor_id']\n        features['sentence'] = fmd['sentence']\n        features['emotion'] = fmd['emotion']\n        features['intensity'] = fmd['intensity']\n        features['audio_duration'] = audio_duration\n        features['sample_rate'] = sr\n        \n        # 1. MFCCs (most important for speech)\n        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n        for i in range(13):\n            features[f'mfcc_{i+1}_mean'] = np.mean(mfccs[i])\n            features[f'mfcc_{i+1}_std'] = np.std(mfccs[i])\n        \n        # 2. Spectral features\n        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n        features['spectral_centroid_std'] = np.std(spectral_centroid)\n        \n        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n        \n        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n        \n        # 3. Energy and rhythm\n        rms = librosa.feature.rms(y=y)\n        features['rms_mean'] = np.mean(rms)\n        features['rms_std'] = np.std(rms)\n      \n        zcr = librosa.feature.zero_crossing_rate(y)\n        features['zcr_mean'] = np.mean(zcr)\n        \n        # 4. Pitch and harmony\n        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n        features['chroma_mean'] = np.mean(chroma)\n        features['chroma_std'] = np.std(chroma)\n        \n        all_features.append(features)\n        \n  return all_features\n\n# Function call code has been commented since the data has been extracted and converted into csv file for consumption.\n# call function, while passing directory of audio files\n#audio_features = extract_features(audio_dir)\n\n# convert to a data frame\n#df = pd.DataFrame(audio_features)\n\n# export dataframe to csv for repoducibility \n#df.to_csv(\"./data/crema_d.csv\")\n\ndf = pd.read_csv(\"./data/crema_d.csv\", index_col=0)\n```\n:::\n\n\n### Dataset\n\n::: {#cell-dataset .cell execution_count=2}\n\n::: {#dataset .cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>actor_id</th>\n      <th>sentence</th>\n      <th>emotion</th>\n      <th>intensity</th>\n      <th>audio_duration</th>\n      <th>sample_rate</th>\n      <th>mfcc_1_mean</th>\n      <th>mfcc_1_std</th>\n      <th>mfcc_2_mean</th>\n      <th>mfcc_2_std</th>\n      <th>...</th>\n      <th>mfcc_13_std</th>\n      <th>spectral_centroid_mean</th>\n      <th>spectral_centroid_std</th>\n      <th>spectral_rolloff_mean</th>\n      <th>spectral_bandwidth_mean</th>\n      <th>rms_mean</th>\n      <th>rms_std</th>\n      <th>zcr_mean</th>\n      <th>chroma_mean</th>\n      <th>chroma_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1022</td>\n      <td>ITS</td>\n      <td>angry</td>\n      <td>XX</td>\n      <td>2.435782</td>\n      <td>22050</td>\n      <td>-266.28894</td>\n      <td>123.935930</td>\n      <td>108.554596</td>\n      <td>39.694670</td>\n      <td>...</td>\n      <td>10.892748</td>\n      <td>1754.476189</td>\n      <td>965.090546</td>\n      <td>3350.053711</td>\n      <td>1701.814132</td>\n      <td>0.096146</td>\n      <td>0.110308</td>\n      <td>0.095815</td>\n      <td>0.328043</td>\n      <td>0.306033</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1037</td>\n      <td>ITS</td>\n      <td>angry</td>\n      <td>XX</td>\n      <td>3.003039</td>\n      <td>22050</td>\n      <td>-346.40980</td>\n      <td>83.344710</td>\n      <td>125.381540</td>\n      <td>42.769300</td>\n      <td>...</td>\n      <td>10.813552</td>\n      <td>1624.501830</td>\n      <td>1058.895061</td>\n      <td>3325.968863</td>\n      <td>1722.048847</td>\n      <td>0.038797</td>\n      <td>0.031771</td>\n      <td>0.091384</td>\n      <td>0.370311</td>\n      <td>0.322135</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1060</td>\n      <td>ITS</td>\n      <td>neutral</td>\n      <td>XX</td>\n      <td>2.402404</td>\n      <td>22050</td>\n      <td>-421.48450</td>\n      <td>36.981285</td>\n      <td>140.371900</td>\n      <td>19.000977</td>\n      <td>...</td>\n      <td>5.230837</td>\n      <td>1406.515534</td>\n      <td>563.173067</td>\n      <td>3186.085862</td>\n      <td>1848.645713</td>\n      <td>0.008990</td>\n      <td>0.005157</td>\n      <td>0.060317</td>\n      <td>0.403894</td>\n      <td>0.303255</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1075</td>\n      <td>ITS</td>\n      <td>neutral</td>\n      <td>XX</td>\n      <td>2.435782</td>\n      <td>22050</td>\n      <td>-413.22550</td>\n      <td>50.740425</td>\n      <td>140.576370</td>\n      <td>28.177567</td>\n      <td>...</td>\n      <td>5.935963</td>\n      <td>1370.133893</td>\n      <td>682.471987</td>\n      <td>3006.958008</td>\n      <td>1780.081965</td>\n      <td>0.011599</td>\n      <td>0.007132</td>\n      <td>0.062360</td>\n      <td>0.414367</td>\n      <td>0.308347</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1073</td>\n      <td>IOM</td>\n      <td>disgust</td>\n      <td>XX</td>\n      <td>2.869569</td>\n      <td>22050</td>\n      <td>-415.93317</td>\n      <td>61.064003</td>\n      <td>136.759430</td>\n      <td>19.811580</td>\n      <td>...</td>\n      <td>6.920853</td>\n      <td>1164.595733</td>\n      <td>422.881734</td>\n      <td>2682.273028</td>\n      <td>1686.101546</td>\n      <td>0.014929</td>\n      <td>0.013538</td>\n      <td>0.041378</td>\n      <td>0.411929</td>\n      <td>0.306035</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 41 columns</p>\n</div>\n```\n:::\n:::\n\n\n### Dataset Information\n\n::: {#dataset-info .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 7442 entries, 0 to 7441\nData columns (total 41 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   actor_id                 7442 non-null   int64  \n 1   sentence                 7442 non-null   object \n 2   emotion                  7442 non-null   object \n 3   intensity                7442 non-null   object \n 4   audio_duration           7442 non-null   float64\n 5   sample_rate              7442 non-null   int64  \n 6   mfcc_1_mean              7442 non-null   float64\n 7   mfcc_1_std               7442 non-null   float64\n 8   mfcc_2_mean              7442 non-null   float64\n 9   mfcc_2_std               7442 non-null   float64\n 10  mfcc_3_mean              7442 non-null   float64\n 11  mfcc_3_std               7442 non-null   float64\n 12  mfcc_4_mean              7442 non-null   float64\n 13  mfcc_4_std               7442 non-null   float64\n 14  mfcc_5_mean              7442 non-null   float64\n 15  mfcc_5_std               7442 non-null   float64\n 16  mfcc_6_mean              7442 non-null   float64\n 17  mfcc_6_std               7442 non-null   float64\n 18  mfcc_7_mean              7442 non-null   float64\n 19  mfcc_7_std               7442 non-null   float64\n 20  mfcc_8_mean              7442 non-null   float64\n 21  mfcc_8_std               7442 non-null   float64\n 22  mfcc_9_mean              7442 non-null   float64\n 23  mfcc_9_std               7442 non-null   float64\n 24  mfcc_10_mean             7442 non-null   float64\n 25  mfcc_10_std              7442 non-null   float64\n 26  mfcc_11_mean             7442 non-null   float64\n 27  mfcc_11_std              7442 non-null   float64\n 28  mfcc_12_mean             7442 non-null   float64\n 29  mfcc_12_std              7442 non-null   float64\n 30  mfcc_13_mean             7442 non-null   float64\n 31  mfcc_13_std              7442 non-null   float64\n 32  spectral_centroid_mean   7442 non-null   float64\n 33  spectral_centroid_std    7442 non-null   float64\n 34  spectral_rolloff_mean    7442 non-null   float64\n 35  spectral_bandwidth_mean  7442 non-null   float64\n 36  rms_mean                 7442 non-null   float64\n 37  rms_std                  7442 non-null   float64\n 38  zcr_mean                 7442 non-null   float64\n 39  chroma_mean              7442 non-null   float64\n 40  chroma_std               7442 non-null   float64\ndtypes: float64(36), int64(2), object(3)\nmemory usage: 2.4+ MB\n```\n:::\n:::\n\n\n### Target Frequency\n\n::: {#018bda77 .cell message='false' execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](proposal_files/figure-html/cell-5-output-1.png){width=676 height=554}\n:::\n:::\n\n\n### Intensity Frequency\n\n::: {#87e5d09a .cell message='false' execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](proposal_files/figure-html/cell-6-output-1.png){width=676 height=535}\n:::\n:::\n\n\n### Spectral Centroid Distribution\n\n::: {#d1453bc8 .cell message='false' execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](proposal_files/figure-html/cell-7-output-1.png){width=676 height=554}\n:::\n:::\n\n\n### Root Mean Square Distribution\n\n::: {#270267b8 .cell message='false' execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](proposal_files/figure-html/cell-8-output-1.png){width=672 height=554}\n:::\n:::\n\n\n### Zero Crossing Rate Distribution\n\n::: {#173495fc .cell message='false' execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](proposal_files/figure-html/cell-9-output-1.png){width=672 height=554}\n:::\n:::\n\n\n### Croma Distribution\n\n::: {#0f7c294e .cell message='false' execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](proposal_files/figure-html/cell-10-output-1.png){width=663 height=554}\n:::\n:::\n\n\n## Research Questions\n\n### Question 1: Basic Emotion Classification\n\n**Can we accurately classify four emotions (happy, sad, angry, fear) from audio features using traditional machine learning algorithms?**<br> This question investigates the fundamental capability of machine learning models to distinguish emotional states through acoustic analysis. By focusing on four distinct emotions, the study maintains sufficient complexity while ensuring manageable scope for comprehensive analysis.\n\n### Question 2: Ensemble Learning Effectiveness\n\n**Can combining multiple machine learning algorithms (ensemble methods) significantly improve emotion classification accuracy compared to individual models, and which ensemble strategies work best for acoustic emotion recognition?**<br> This question explores the relationship between emotional expression intensity and model performance, investigating whether stronger emotional expressions are easier to classify and whether audio features can predict when models will be confident in their predictions.\n\n## Analysis Plan\n\n### Question 1: Basic Emotion Classification\n\n**Target Variable:** `emotion` (4-class: happy, sad, angry, fear)\n\n**Feature Extraction Strategy:** The analysis employs five key acoustic feature categories using the librosa library:\n\n**1. Spectral Contrast:** Measures amplitude differences between spectral peaks and valleys, capturing timbral characteristics that distinguish emotional expressions\n\n**2. MFCCs (Mel-frequency cepstral coefficients):** Extract 13 coefficients representing the short-term power spectrum, fundamental for speech emotion recognition\n\n**3. Chroma Features:** Capture pitch class energy distribution, providing harmonic content information relevant to emotional prosody\n\n**4. Zero-Crossing Rate:** Quantifies signal noisiness by measuring zero-axis crossings, distinguishing between voiced and unvoiced speech segments\n\n**5. Root Mean Square (RMS) Energy:** Measures overall signal energy, correlating with loudness and emotional intensity\n\n**Model Implementation:** Three complementary algorithms will be implemented and compared:\n\n| **Model** | **Description** |\n|--------------------|----------------------------------------------------|\n| **Logistic Regression** | Provides interpretable baseline with coefficient analysis for feature importance |\n| **Random Forest** | Offers robust performance with built-in feature importance rankings and handling of non-linear relationships |\n| **Support Vector Machine** | Excels with high-dimensional feature spaces common in audio analysis |\n\n**Evaluation Framework:** Models will be assessed using train/test split methodology with comprehensive metrics including accuracy, precision, recall, and F1-score for each emotion class, providing detailed performance analysis across emotional categories.\n\n### Question 2: Ensemble Learning Effectiveness\n\n**Variables Involved:**\n\n-   All audio features from Question 1 (spectral contrast, MFCCs, chroma, ZCR, RMS energy)\n-   `individual_predictions`: Predictions from each base model (LR, RF, SVM)\n-   `ensemble_prediction`: Combined prediction from ensemble methods\n-   `individual_confidence`: Confidence scores (prediction probabilities) from each model\n-   `ensemble_confidence`: Combined confidence measures from ensemble approaches\n\n**Analysis Approach:**\n\n**Base Model Training:** Train all three algorithms (Logistic Regression, Random Forest, SVM) separately using identical feature sets and training data, establishing baseline performance metrics for each individual approach.\n\n**Ensemble Creation:** Implement multiple ensemble strategies including hard voting (majority vote), soft voting (probability-based), and simple probability averaging to combine individual model predictions and assess different aggregation approaches.\n\n**Performance Comparison:** Conduct systematic comparison of individual model accuracy against ensemble methods using cross-validation, statistical significance testing, and detailed performance metrics to quantify improvement gains.\n\n**Feature Analysis:** Investigate which acoustic features contribute most effectively to each algorithm's performance, identifying complementary strengths that ensemble methods can exploit for improved classification accuracy.\n\n## Technical Implementation\n\n**Programming Environment:** Python with scientific computing stack\n\n#### Key Libraries\n\n| **Library**              | **Purpose**                                 |\n|--------------------------|---------------------------------------------|\n| `librosa`                | Audio feature extraction and processing     |\n| `scikit-learn`           | Machine learning algorithms and evaluation  |\n| `pandas` / `numpy`       | Data manipulation and numerical computation |\n| `matplotlib` / `seaborn` | Visualization and results presentation      |\n\n## Expected Project Timeline (3\\~ weeks)\n\n#### Week 0: Data preparation and exploration\n\n-   Download CREMA-D from Kaggle (\\~2GB)\n-   Explore dataset structure and file naming\n-   Proposal write-up and review\n\n#### Week 1: Feature extraction and dataset creation\n\n-   Implement feature extraction pipeline\n-   Process selected audio files\n-   Create clean feature dataset\n-   Exploratory data analysis of features vs emotions\n\n#### Week 2: Individual model development\n\n-   Train baseline models (Logistic Regression, Decision Tree, SVM)\n-   Hyperparameter tuning using GridSearchCV\n-   Performance evaluation and comparison\n-   Feature importance analysis\n\n#### Week 3: Ensemble methods and final analysis\n\n-   Implement ensemble approaches\n-   Compare individual vs ensemble performance\n-   Statistical significance testing\n-   Final report and presentation\n\n## Project Structure\n\n| Folder / File Name | Description |\n|---------------------------------------------|---------------------------|\n| `.quarto/` | Quarto's internal folder—automatically created to manage rendering settings and cache. You typically don't touch this. |\n| `_extra/` | Stores supporting materials that aren't part of the main outputs but are useful for context. |\n| `_freeze/` | Keeps locked-in versions of documents to ensure consistency when rebuilding or sharing. |\n| `_site/` | Final output folder generated after rendering; includes the HTML version of your site. |\n| `data/` | Where all the data lives—raw inputs, cleaned datasets, and a README explaining structure and sources. |\n| `images/` | Used for storing visual content like charts, graphs, and illustrations referenced in your `.qmd` files. |\n| `style/` | Contains custom design elements, like SCSS files, that control the look and feel of your site. |\n| `index.qmd` | Acts as the homepage, giving a snapshot of what the project is about. |\n| `about.qmd` | Gives extra context—background info, author bio, or detailed project narrative. |\n| `proposal.qmd` | The full research plan: includes goals, methods, schedule, and how everything is structured. |\n| `presentation.qmd` | Slide deck made with Quarto to highlight the most important insights from your project. |\n\n##\n\n",
    "supporting": [
      "proposal_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}