---
title: "Acoustic Emotion Classification"
subtitle: "Optimizing Performance Through Ensemble Methods"
author: 
  - name: "Ralp Andrade"
    affiliations:
      - name: "College of Information Science, University of Arizona"
description: "An emotion classification project using machine learning to identify four emotions (happy, sad, angry, fear) from speech audio recordings, comparing different algorithms to determine the most effective approach for acoustic emotion recognition."
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Abstract
This project investigates the fundamental capability of machine learning models to distinguish emotional states through acoustic feature analysis. Using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset), our study focuses on the classification emotions: neutral, happy, sad, angry, fear, disgust. A combination of traditional feature engineering paired with ensemble learning methods forms the basis for developing robust emotion recognition models. We systematically examine and compare the predictive performance of individual algorithms with ensemble and neural network strategies for multi-class emotion recognition.

## Introduction

Building on the challenge of automatic emotion classification, this project leverages the Crowd-Sourced Emotional Multimodal Actors Dataset (CREMA-D), a widely recognized collection of acted speech designed for emotion research. The dataset comprises 7,442 `.wav` clips from 91 actors portraying six basic emotions. Existing research shows that while audio features like MFCCs and spectral properties are often used for emotion detection, there is still no agreement on the best feature selection or model designs, especially in traditional machine learning contexts. (Banerjee, Huang, & Lettiere, n.d.). We transformed raw audio signals into quantitative features using the `librosa` library, as detailed in our [proposal](https://info-523-su25.github.io/final-project-etlflow/proposal.html).

Project's goal is to explore the effectiveness of unsupervised learning for predicting multi-class emotion targets, aiming to benchmark these results from ensemble strategies against standalone and neural network models. This methodology offers new perspective on the relative utility of unsupervised techniques in speech emotion recognition.

## Research Question
-   **Q1. What is the classification accuracy of unsupervised methods for emotion recognition from acoustic features?**
-   **Q2. How do standalone algorithms, ensemble approaches, and neural networks compare in terms of accuracy, robustness, and computational efficiency for emotion classification from audio?**

# Exploratory Analysis
## Inital data load & Observations
```{python}
#| label: dataset
#| echo: true

# Import libs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PowerTransformer, LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split

# Import required libraries for models and metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (classification_report, confusion_matrix, 
                           accuracy_score, precision_score, recall_score, f1_score)

#import data into df
df = pd.read_csv("./data/crema_d.csv", index_col=0)

# number of variables and observations in the data
print(f"Total observations: {df.shape[0]}")
print(f"Number of features: {df.shape[1]}")
```

