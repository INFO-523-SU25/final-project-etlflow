---
title: "Acoustic Emotion Classification"
subtitle: "Optimizing Performance Through Ensemble Methods"
author: 
  - name: "Ralp Andrade"
    affiliations:
      - name: "College of Information Science, University of Arizona"
description: "An emotion classification project using machine learning to identify four emotions (happy, sad, angry, fear) from speech audio recordings, comparing different algorithms to determine the most effective approach for acoustic emotion recognition."
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Abstract
This project investigates the fundamental capability of machine learning models to distinguish emotional states through acoustic feature analysis. Using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset), our study focuses on the classification emotions: neutral, happy, sad, angry, fear, disgust. A combination of traditional feature engineering paired with ensemble learning methods forms the basis for developing robust emotion recognition models. We systematically examine and compare the predictive performance of individual algorithms with ensemble and neural network strategies for multi-class emotion recognition.

## Introduction

Building on the challenge of automatic emotion classification, this project leverages the Crowd-Sourced Emotional Multimodal Actors Dataset (CREMA-D), a widely recognized collection of acted speech designed for emotion research. The dataset comprises 7,442 `.wav` clips from 91 actors portraying six basic emotions. Existing research shows that while audio features like MFCCs and spectral properties are often used for emotion detection, there is still no agreement on the best feature selection or model designs, especially in traditional machine learning contexts. (Banerjee, Huang, & Lettiere, n.d.). We transformed raw audio signals into quantitative features using the `librosa` library, as detailed in our [proposal](https://info-523-su25.github.io/final-project-etlflow/proposal.html).

Project's goal is to explore the effectiveness of unsupervised learning for predicting multi-class emotion targets, aiming to benchmark these results from ensemble strategies against standalone and neural network models. This methodology offers new perspective on the relative utility of unsupervised techniques in speech emotion recognition.

## Research Question
-   **Q1. What is the classification accuracy of unsupervised methods for emotion recognition from acoustic features?**
-   **Q2. How do standalone algorithms, ensemble approaches, and neural networks compare in terms of accuracy, robustness, and computational efficiency for emotion classification from audio?**

# Exploratory Analysis
## Inital data load & Observations
```{python}
#| label: dataset
#| echo: true

# Import libs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PowerTransformer, LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split

# Import required libraries for models and metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (classification_report, confusion_matrix, 
                           accuracy_score, precision_score, recall_score, f1_score)

#import data into df
df = pd.read_csv("./data/crema_d.csv", index_col=0)

# number of variables and observations in the data
print(f"Total observations: {df.shape[0]}")
print(f"Number of features: {df.shape[1]}")
```

### Target Class Distribution

Since our projects goal is to predict mutiple classes, we need to determine if a class imbalance exists. Our aim for this project is to focus on predicting four (happy, sad, angry, fear) out of the six emotions in the dataset, as we intend to absorb the remainder (neutral , disgust) as part of the traning set.

```{python}
#| echo: false
#| warning: false
#| message: false

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='emotion', hue='emotion')
plt.title("Target Counts")
plt.xticks(rotation=45)
plt.show()

# Show class distribution for all emotions
emotion_counts = df['emotion'].value_counts()
print("Class distribution:\n", emotion_counts)

# Focus on the four target emotions
target_emotions = ['happy', 'sad', 'angry', 'fear', 'neutral','disgust']
target_counts = df[df['emotion'].isin(target_emotions)]['emotion'].value_counts()
print("\nTarget emotion counts:\n", target_counts)
```

# Data Preprocessing
## Data Cleaning
Since the data contains both positive, zero, and negative skew, we are going to use the Yeo Johnson power transformer to make sure features are standardized.

```{python}
#| echo: false
#| warning: false
#| message: false

# missing values in each column
missing_df = df.isna().sum()
print("Missing values per column:\n", missing_df)

# Remove unwanted columns
df = df.drop(columns=['actor_id', 'sentence', 'intensity','sample_rate'])

#Indentify skew of numeric cols
num_cols = df.select_dtypes(include='number').columns
print(num_cols)

skewness_before = df[num_cols].skew().sort_values(ascending=False)
print("Before:", skewness_before)

# yeo-johnson power transformer
yeojt = PowerTransformer(method='yeo-johnson', standardize=True)
df_transformed = yeojt.fit_transform(df[num_cols])

# Create DataFrame from transformed data with correct columns
df_t = pd.DataFrame(df_transformed, columns=num_cols)

# Overwrite original columns in df with transformed values
df[num_cols] = df_t

# Get skewness after transformation
skewness_after = df[num_cols].skew().sort_values(ascending=False)
print("After:", skewness_after)

# Combined before/after skewness chart
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Before transformation
sns.barplot(x=skewness_before.values, y=skewness_before.index, ax=ax1, color='lightcoral')
ax1.set_title("Skewness Before Yeo-Johnson", fontsize=14)
ax1.axvline(x=0, color='red', linestyle='--', alpha=0.7)
ax1.set_xlabel("Skewness", fontsize=12)

# After transformation
sns.barplot(x=skewness_after.values, y=skewness_after.index, ax=ax2, color='lightblue')
ax2.set_title("Skewness After Yeo-Johnson", fontsize=14)
ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)
ax2.set_xlabel("Skewness", fontsize=12)

# Adjust layout
plt.tight_layout()
plt.show()

# encode target variable
le = LabelEncoder()
df['encoded_emotion'] = le.fit_transform(df['emotion'])

# View mapping
print(dict(zip(le.classes_, le.transform(le.classes_))))
```

### Splitting the dataset

Since we are absorbing neutral and disgust records into the training split, we have split the data into target and omitted data frames. The train test split is performed on the target data frame, while the omitted data frame is concatenated back into the target after the split.

```{python}
# train/test split for target emotions
X = df.drop(columns=['emotion','encoded_emotion'])
y = df['encoded_emotion']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

```

### Data Scaling and Dimension Reduction

Data is scaled using the standard scaler on all numeric features that are part of the X_train, and X_test split. After scaling, we perform Principal Component Analysis to determine the number of dimensions while retaining 95% variance.

```{python}
# select all numeric columns
num_cols = X_train.select_dtypes(include='number').columns

# Apply standard scaler on numeric data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[num_cols])
X_test_scaled = scaler.transform(X_test[num_cols])

# Apply PCA while retaining 90
pca = PCA(n_components=.95)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Explained variance ratio
explained_variance = pca.explained_variance_ratio_

# Create scree plot
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance)+1), explained_variance, 'o-', linewidth=2, color='blue')
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.xticks(range(1, len(explained_variance)+1))
plt.grid(True)
plt.show()

# Reapply PCA Components from scree plot
pca = PCA()
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)
```

### Model Traning and Evalution

```{python}
# Skip extending y_train - we want to exclude omitted emotions from predictions
# Use original y_train that only contains target emotions

# Get emotion labels for reporting
emotion_labels = [emotion for emotion in le.classes_ if emotion in target_emotions]
target_label_mapping = {le.transform([emotion])[0]: emotion for emotion in emotion_labels}

print("Target emotion label mapping:", target_label_mapping)

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    """
    Train and evaluate a model with comprehensive metrics
    """
    print(f"\n{'='*50}")
    print(f"Training {model_name}")
    print(f"{'='*50}")
    
    # Train model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Overall metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)
    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)
    
    print(f"\n{model_name} Overall Performance:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Macro Precision: {precision_macro:.4f}")
    print(f"Macro Recall: {recall_macro:.4f}")
    print(f"Macro F1-Score: {f1_macro:.4f}")
    
    # Per-class metrics
    print(f"\n{model_name} Per-Class Performance:")
    target_names = [target_label_mapping.get(i, f'Class_{i}') for i in sorted(np.unique(y_test))]
    
    report = classification_report(y_test, y_pred, 
                                 target_names=target_names,
                                 output_dict=True, 
                                 zero_division=0)
    
    # Display per-class metrics in a formatted way
    print(f"{'Emotion':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
    print("-" * 50)
    
    for emotion in target_names:
        if emotion in report:
            metrics = report[emotion]
            print(f"{emotion:<10} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} "
                  f"{metrics['f1-score']:<10.4f} {int(metrics['support']):<10}")
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=target_names, yticklabels=target_names)
    plt.title(f'{model_name} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    
    # Return results for comparison
    return {
        'model_name': model_name,
        'accuracy': accuracy,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'per_class_report': report
    }

# Initialize models
# random forest
rf_model = RandomForestClassifier(
    n_estimators=1000,           # More trees for better performance
    max_depth=15,               # Prevent overfitting while allowing complexity
    min_samples_split=5,        # Require more samples to split
    min_samples_leaf=2,         # Minimum samples in leaf nodes
    max_features='sqrt',        # Feature sampling at each split
    bootstrap=True,             # Bootstrap sampling
    oob_score=True,             # Out-of-bag scoring
    n_jobs=-1,                  # Use all available cores
    random_state=42
)
# support vector machine
svm_model = SVC(
    kernel='rbf',               # RBF kernel works well for most cases
    C=10.0,                     # Regularization parameter
    gamma='scale',              # Kernel coefficient (auto-scaled)
    probability=True,           # Enable probability estimates
    cache_size=1000,            # Increase cache for faster training
    class_weight='balanced',    # Handle imbalanced datasets
    random_state=42
)

# standard neral network 
mlp_model = MLPClassifier(
    hidden_layer_sizes=(128, 64, 32), 
    max_iter=1000,
    random_state=42, 
    early_stopping=True, 
    validation_fraction=0.1
    )

# Evaluate ensemble models along with stand alone models
results = []

# Random Forest
rf_results = evaluate_model(rf_model, X_train_pca, X_test_pca, 
                           y_train, y_test, "Random Forest")
results.append(rf_results)

# SVM
svm_results = evaluate_model(svm_model, X_train_pca, X_test_pca, 
                            y_train, y_test, "SVM")
results.append(svm_results)

# MLP
mlp_results = evaluate_model(mlp_model, X_train_pca, X_test_pca, 
                            y_train, y_test, "MLP(Neural Net)")
results.append(mlp_results)
```
