---
title: "Acoustic Emotion Classification"
subtitle: "Optimizing Performance Through Ensemble Methods"
author: 
  - name: "Ralp Andrade"
    affiliations:
      - name: "College of Information Science, University of Arizona"
description: "An emotion classification project using machine learning to identify four emotions (happy, sad, angry, fear) from speech audio recordings, comparing different algorithms to determine the most effective approach for acoustic emotion recognition."
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Abstract
This project investigates the fundamental capability of machine learning models to distinguish emotional states through acoustic feature analysis. Using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset), our study focuses on the classification emotions: neutral, happy, sad, angry, fear, disgust. A combination of traditional feature engineering paired with ensemble learning methods forms the basis for developing robust emotion recognition models. We systematically examine and compare the predictive performance of individual algorithms with ensemble and neural network strategies for multi-class emotion recognition.

## Introduction

Building on the challenge of automatic emotion classification, this project leverages the Crowd-Sourced Emotional Multimodal Actors Dataset (CREMA-D), a widely recognized collection of acted speech designed for emotion research. The dataset comprises 7,442 `.wav` clips from 91 actors portraying six basic emotions. Existing research shows that while audio features like MFCCs and spectral properties are often used for emotion detection, there is still no agreement on the best feature selection or model designs, especially in traditional machine learning contexts. (Banerjee, Huang, & Lettiere, n.d.). We transformed raw audio signals into quantitative features using the `librosa` library, as detailed in our [proposal](https://info-523-su25.github.io/final-project-etlflow/proposal.html).

Project's goal is to explore the effectiveness of unsupervised learning for predicting multi-class emotion targets, aiming to benchmark these results from ensemble strategies against standalone and neural network models. This methodology offers new perspective on the relative utility of unsupervised techniques in speech emotion recognition.

## Research Question
-   **Q1. What is the classification accuracy of unsupervised methods for emotion recognition from acoustic features?**
-   **Q2. How do standalone algorithms, ensemble approaches, and neural networks compare in terms of accuracy, robustness, and computational efficiency for emotion classification from audio?**

# Exploratory Analysis
## Inital data load & Observations
```{python}
#| label: dataset
#| echo: true

# Import libs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PowerTransformer, LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split

# Import required libraries for models and metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (classification_report, confusion_matrix, 
                           accuracy_score, precision_score, recall_score, f1_score)

#import data into df
df = pd.read_csv("./data/crema_d.csv", index_col=0)

# number of variables and observations in the data
print(f"Total observations: {df.shape[0]}")
print(f"Number of features: {df.shape[1]}")
```

### Target Class Distribution

Since our projects goal is to predict mutiple classes, we need to determine if a class imbalance exists. Our aim for this project is to focus on predicting four (happy, sad, angry, fear) out of the six emotions in the dataset, as we intend to absorb the remainder (neutral , disgust) as part of the traning set.

```{python}
#| echo: false
#| warning: false
#| message: false

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='emotion', hue='emotion')
plt.title("Target Counts")
plt.xticks(rotation=45)
plt.show()

# Show class distribution for all emotions
emotion_counts = df['emotion'].value_counts()
print("Class distribution:\n", emotion_counts)

# Focus on the four target emotions
target_emotions = ['happy', 'sad', 'angry', 'fear', 'neutral','disgust']
target_counts = df[df['emotion'].isin(target_emotions)]['emotion'].value_counts()
print("\nTarget emotion counts:\n", target_counts)
```

# Data Preprocessing
## Data Cleaning
Since the data contains both positive, zero, and negative skew, we are going to use the Yeo Johnson power transformer to make sure features are standardized.

```{python}
#| echo: false
#| warning: false
#| message: false

# missing values in each column
missing_df = df.isna().sum()
print("Missing values per column:\n", missing_df)

# Remove unwanted columns
df = df.drop(columns=['actor_id', 'sentence', 'intensity','sample_rate'])

#Indentify skew of numeric cols
num_cols = df.select_dtypes(include='number').columns
print(num_cols)

skewness_before = df[num_cols].skew().sort_values(ascending=False)
print("Before:", skewness_before)

# yeo-johnson power transformer
yeojt = PowerTransformer(method='yeo-johnson', standardize=True)
df_transformed = yeojt.fit_transform(df[num_cols])

# Create DataFrame from transformed data with correct columns
df_t = pd.DataFrame(df_transformed, columns=num_cols)

# Overwrite original columns in df with transformed values
df[num_cols] = df_t

# Get skewness after transformation
skewness_after = df[num_cols].skew().sort_values(ascending=False)
print("After:", skewness_after)

# Combined before/after skewness chart
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Before transformation
sns.barplot(x=skewness_before.values, y=skewness_before.index, ax=ax1, color='lightcoral')
ax1.set_title("Skewness Before Yeo-Johnson", fontsize=14)
ax1.axvline(x=0, color='red', linestyle='--', alpha=0.7)
ax1.set_xlabel("Skewness", fontsize=12)

# After transformation
sns.barplot(x=skewness_after.values, y=skewness_after.index, ax=ax2, color='lightblue')
ax2.set_title("Skewness After Yeo-Johnson", fontsize=14)
ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)
ax2.set_xlabel("Skewness", fontsize=12)

# Adjust layout
plt.tight_layout()
plt.show()

# encode target variable
le = LabelEncoder()
df['encoded_emotion'] = le.fit_transform(df['emotion'])

# View mapping
print(dict(zip(le.classes_, le.transform(le.classes_))))
```

### Splitting the dataset

Since we are absorbing neutral and disgust records into the training split, we have split the data into target and omitted data frames. The train test split is performed on the target data frame, while the omitted data frame is concatenated back into the target after the split.

```{python}
# train/test split for target emotions
X = df.drop(columns=['emotion','encoded_emotion'])
y = df['encoded_emotion']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

```

### Data Scaling and Dimension Reduction

Data is scaled using the standard scaler on all numeric features that are part of the X_train, and X_test split. After scaling, we perform Principal Component Analysis to determine the number of dimensions while retaining 95% variance.

```{python}
# select all numeric columns
num_cols = X_train.select_dtypes(include='number').columns

# Apply standard scaler on numeric data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[num_cols])
X_test_scaled = scaler.transform(X_test[num_cols])

# Apply PCA while retaining 90
pca = PCA(n_components=.95)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Explained variance ratio
explained_variance = pca.explained_variance_ratio_

# Create scree plot
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance)+1), explained_variance, 'o-', linewidth=2, color='blue')
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.xticks(range(1, len(explained_variance)+1))
plt.grid(True)
plt.show()

# Reapply PCA Components from scree plot
pca = PCA()
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)
```
