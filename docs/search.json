[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Acoustic Emotion Classification",
    "section": "",
    "text": "This project investigates the application of machine learning techniques to classify emotions from speech audio using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset). The study focuses on distinguishing four primary emotions—happy, sad, angry, and fear—through acoustic feature analysis and explores how emotional intensity affects classification performance. By combining traditional feature engineering with ensemble learning methods, this research aims to develop robust emotion recognition models while examining the relationship between speech intensity and model confidence.\nThe project addresses two fundamental questions in acoustic emotion recognition: first, whether traditional machine learning algorithms can accurately classify emotions using engineered audio features, and second, how ensemble methods can significantly improve classification performance compared to individual models. This investigation contributes to the growing field of effective computing while demonstrating practical applications of ensemble learning principles in speech emotion recognition."
  },
  {
    "objectID": "proposal.html#project-overview",
    "href": "proposal.html#project-overview",
    "title": "Acoustic Emotion Classification",
    "section": "",
    "text": "This project investigates the application of machine learning techniques to classify emotions from speech audio using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset). The study focuses on distinguishing four primary emotions—happy, sad, angry, and fear—through acoustic feature analysis and explores how emotional intensity affects classification performance. By combining traditional feature engineering with ensemble learning methods, this research aims to develop robust emotion recognition models while examining the relationship between speech intensity and model confidence.\nThe project addresses two fundamental questions in acoustic emotion recognition: first, whether traditional machine learning algorithms can accurately classify emotions using engineered audio features, and second, how ensemble methods can significantly improve classification performance compared to individual models. This investigation contributes to the growing field of effective computing while demonstrating practical applications of ensemble learning principles in speech emotion recognition."
  },
  {
    "objectID": "proposal.html#dataset-description",
    "href": "proposal.html#dataset-description",
    "title": "Acoustic Emotion Classification",
    "section": "Dataset Description",
    "text": "Dataset Description\nPrimary Dataset: CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset) Provenance: CREMA-D is a validated multi modal database created through collaboration between researchers and crowd sourced validation. The dataset contains emotional speech recordings from professional actors, making it ideal for supervised learning approaches to emotion classification.\n\nDimensions & Structure:\n\n\n\n\n\n\n\nAttribute\nDetails\n\n\n\n\nTotal Files\n7,442 audio clips from diverse emotional expressions\n\n\nSpeakers\n91 professional actors (48 male, 43 female)\n\n\nAge Range\n20–74 years, providing demographic diversity\n\n\nTarget Emotions\n4 emotions selected for analysis (happy, sad, angry, fear)\n\n\nEmotional Intensities\nMultiple intensity levels (low, medium, high, unspecified)\n\n\nFile Format\nWAV files suitable for feature extraction\n\n\nSentence Variety\n12 different sentences to reduce linguistic bias\n\n\n\nDataset Selection Rationale: CREMA-D was chosen for its substantial size, demographic diversity, and established use in emotion recognition research. The dataset’s systematic organization and intensity labels directly support both research questions, while its availability through Kaggle ensures reproducible research practices."
  },
  {
    "objectID": "proposal.html#signal-processing-extract-transform-load",
    "href": "proposal.html#signal-processing-extract-transform-load",
    "title": "Acoustic Emotion Classification",
    "section": "Signal Processing (Extract, Transform, Load)",
    "text": "Signal Processing (Extract, Transform, Load)\nThis pipeline enables the conversion of complex, high-dimensional waveforms into compact, informative representations—such as MFCCs or spectral centroids—that capture the essence of the sound.\n\n# Import libs\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport os\n\n# CREMA-D filename structure: ActorID_SentenceID_EmotionID_IntensityID.wav\naudio_dir = \"./data/CREMA_D/\"\n\ndef parse_cremad_filename(file_path):\n  name = os.path.splitext(os.path.basename(file_path))[0]\n  parts = name.split('_')\n  actor_id = parts[0]\n  sentence_id = parts[1]\n  emotion = parts[2]\n  intensity = parts[3].split('.')[0]\n  \n  # mapping emotion_id to emotions\n  emotion_map = {\"ANG\":\"angry\"\n  ,\"DIS\":\"disgust\"\n  ,\"FEA\":\"fear\"\n  ,\"HAP\":\"happy\"\n  ,\"NEU\":\"neutral\"\n  ,\"SAD\":\"sad\"}\n  \n  return {\n    \"actor_id\": actor_id\n    ,\"sentence\": sentence_id\n    ,\"emotion\": emotion_map[emotion]\n    ,\"intensity\": intensity\n  }\n\ndef extract_features(file_path):\n  \"\"\"\n    Iteratively extracts audio features from a given .wav file using librosa.\n\n    Features:\n        - Zero Crossing Rate\n        - Chroma STFT\n        - MFCCs\n        - Root Mean Square Energy\n        - Spectral Centroid\n\n    Returns:\n        list of dict: Each dictionary containing the features, and metadata parsed from the filename.\n  \"\"\"\n  # initalize empty list to store all features\n  all_features = []\n  \n  # iteratively scan os directory\n  for entry in os.scandir(file_path):\n    if entry.is_file():\n        # load .wav files iteratively from entry path\n        y, sr = librosa.load(entry.path, sr=22050)\n        \n        # initiallize empty dict\n        features = {}\n        \n        # 0. File Metadata\n        fmd = parse_cremad_filename(entry.name)\n        audio_duration = len(y)/ sr\n        features['actor_id'] = fmd['actor_id']\n        features['sentence'] = fmd['sentence']\n        features['emotion'] = fmd['emotion']\n        features['intensity'] = fmd['intensity']\n        features['audio_duration'] = audio_duration\n        features['sample_rate'] = sr\n        \n        # 1. MFCCs (most important for speech)\n        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n        for i in range(13):\n            features[f'mfcc_{i+1}_mean'] = np.mean(mfccs[i])\n            features[f'mfcc_{i+1}_std'] = np.std(mfccs[i])\n        \n        # 2. Spectral features\n        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n        features['spectral_centroid_std'] = np.std(spectral_centroid)\n        \n        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n        \n        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n        \n        # 3. Energy and rhythm\n        rms = librosa.feature.rms(y=y)\n        features['rms_mean'] = np.mean(rms)\n        features['rms_std'] = np.std(rms)\n      \n        zcr = librosa.feature.zero_crossing_rate(y)\n        features['zcr_mean'] = np.mean(zcr)\n        \n        # 4. Pitch and harmony\n        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n        features['chroma_mean'] = np.mean(chroma)\n        features['chroma_std'] = np.std(chroma)\n        \n        all_features.append(features)\n        \n  return all_features\n\n# Function call code has been commented since the data has been extracted and converted into csv file for consumption.\n# call function, while passing directory of audio files\n#audio_features = extract_features(audio_dir)\n\n# convert to a data frame\n#df = pd.DataFrame(audio_features)\n\n# export dataframe to csv for repoducibility \n#df.to_csv(\"./data/crema_d.csv\")\n\ndf = pd.read_csv(\"./data/crema_d.csv\", index_col=0)\n\n\nDataset\n\n\n\n\n\n\n\n\n\nactor_id\nsentence\nemotion\nintensity\naudio_duration\nsample_rate\nmfcc_1_mean\nmfcc_1_std\nmfcc_2_mean\nmfcc_2_std\n...\nmfcc_13_std\nspectral_centroid_mean\nspectral_centroid_std\nspectral_rolloff_mean\nspectral_bandwidth_mean\nrms_mean\nrms_std\nzcr_mean\nchroma_mean\nchroma_std\n\n\n\n\n0\n1022\nITS\nangry\nXX\n2.435782\n22050\n-266.28894\n123.935930\n108.554596\n39.694670\n...\n10.892748\n1754.476189\n965.090546\n3350.053711\n1701.814132\n0.096146\n0.110308\n0.095815\n0.328043\n0.306033\n\n\n1\n1037\nITS\nangry\nXX\n3.003039\n22050\n-346.40980\n83.344710\n125.381540\n42.769300\n...\n10.813552\n1624.501830\n1058.895061\n3325.968863\n1722.048847\n0.038797\n0.031771\n0.091384\n0.370311\n0.322135\n\n\n2\n1060\nITS\nneutral\nXX\n2.402404\n22050\n-421.48450\n36.981285\n140.371900\n19.000977\n...\n5.230837\n1406.515534\n563.173067\n3186.085862\n1848.645713\n0.008990\n0.005157\n0.060317\n0.403894\n0.303255\n\n\n3\n1075\nITS\nneutral\nXX\n2.435782\n22050\n-413.22550\n50.740425\n140.576370\n28.177567\n...\n5.935963\n1370.133893\n682.471987\n3006.958008\n1780.081965\n0.011599\n0.007132\n0.062360\n0.414367\n0.308347\n\n\n4\n1073\nIOM\ndisgust\nXX\n2.869569\n22050\n-415.93317\n61.064003\n136.759430\n19.811580\n...\n6.920853\n1164.595733\n422.881734\n2682.273028\n1686.101546\n0.014929\n0.013538\n0.041378\n0.411929\n0.306035\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n\nDataset Information\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 7442 entries, 0 to 7441\nData columns (total 41 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   actor_id                 7442 non-null   int64  \n 1   sentence                 7442 non-null   object \n 2   emotion                  7442 non-null   object \n 3   intensity                7442 non-null   object \n 4   audio_duration           7442 non-null   float64\n 5   sample_rate              7442 non-null   int64  \n 6   mfcc_1_mean              7442 non-null   float64\n 7   mfcc_1_std               7442 non-null   float64\n 8   mfcc_2_mean              7442 non-null   float64\n 9   mfcc_2_std               7442 non-null   float64\n 10  mfcc_3_mean              7442 non-null   float64\n 11  mfcc_3_std               7442 non-null   float64\n 12  mfcc_4_mean              7442 non-null   float64\n 13  mfcc_4_std               7442 non-null   float64\n 14  mfcc_5_mean              7442 non-null   float64\n 15  mfcc_5_std               7442 non-null   float64\n 16  mfcc_6_mean              7442 non-null   float64\n 17  mfcc_6_std               7442 non-null   float64\n 18  mfcc_7_mean              7442 non-null   float64\n 19  mfcc_7_std               7442 non-null   float64\n 20  mfcc_8_mean              7442 non-null   float64\n 21  mfcc_8_std               7442 non-null   float64\n 22  mfcc_9_mean              7442 non-null   float64\n 23  mfcc_9_std               7442 non-null   float64\n 24  mfcc_10_mean             7442 non-null   float64\n 25  mfcc_10_std              7442 non-null   float64\n 26  mfcc_11_mean             7442 non-null   float64\n 27  mfcc_11_std              7442 non-null   float64\n 28  mfcc_12_mean             7442 non-null   float64\n 29  mfcc_12_std              7442 non-null   float64\n 30  mfcc_13_mean             7442 non-null   float64\n 31  mfcc_13_std              7442 non-null   float64\n 32  spectral_centroid_mean   7442 non-null   float64\n 33  spectral_centroid_std    7442 non-null   float64\n 34  spectral_rolloff_mean    7442 non-null   float64\n 35  spectral_bandwidth_mean  7442 non-null   float64\n 36  rms_mean                 7442 non-null   float64\n 37  rms_std                  7442 non-null   float64\n 38  zcr_mean                 7442 non-null   float64\n 39  chroma_mean              7442 non-null   float64\n 40  chroma_std               7442 non-null   float64\ndtypes: float64(36), int64(2), object(3)\nmemory usage: 2.4+ MB\n\n\n\n\nTarget Frequency\n\n\n\n\n\n\n\n\n\n\n\nIntensity Frequency\n\n\n\n\n\n\n\n\n\n\n\nSpectral Centroid Distribution\n\n\n\n\n\n\n\n\n\n\n\nRoot Mean Square Distribution\n\n\n\n\n\n\n\n\n\n\n\nZero Crossing Rate Distribution\n\n\n\n\n\n\n\n\n\n\n\nCroma Distribution"
  },
  {
    "objectID": "proposal.html#research-questions",
    "href": "proposal.html#research-questions",
    "title": "Acoustic Emotion Classification",
    "section": "Research Questions",
    "text": "Research Questions\n\nQuestion 1: Basic Emotion Classification\nCan we accurately classify four emotions (happy, sad, angry, fear) from audio features using traditional machine learning algorithms? This question investigates the fundamental capability of machine learning models to distinguish emotional states through acoustic analysis. By focusing on four distinct emotions, the study maintains sufficient complexity while ensuring manageable scope for comprehensive analysis.\n\n\nQuestion 2: Ensemble Learning Effectiveness\nCan combining multiple machine learning algorithms (ensemble methods) significantly improve emotion classification accuracy compared to individual models, and which ensemble strategies work best for acoustic emotion recognition? This question explores the relationship between emotional expression intensity and model performance, investigating whether stronger emotional expressions are easier to classify and whether audio features can predict when models will be confident in their predictions."
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Acoustic Emotion Classification",
    "section": "Analysis Plan",
    "text": "Analysis Plan\n\nQuestion 1: Basic Emotion Classification\nTarget Variable: emotion (4-class: happy, sad, angry, fear)\nFeature Extraction Strategy: The analysis employs five key acoustic feature categories using the librosa library:\n1. Spectral Contrast: Measures amplitude differences between spectral peaks and valleys, capturing timbral characteristics that distinguish emotional expressions\n2. MFCCs (Mel-frequency cepstral coefficients): Extract 13 coefficients representing the short-term power spectrum, fundamental for speech emotion recognition\n3. Chroma Features: Capture pitch class energy distribution, providing harmonic content information relevant to emotional prosody\n4. Zero-Crossing Rate: Quantifies signal noisiness by measuring zero-axis crossings, distinguishing between voiced and unvoiced speech segments\n5. Root Mean Square (RMS) Energy: Measures overall signal energy, correlating with loudness and emotional intensity\nModel Implementation: Three complementary algorithms will be implemented and compared:\n\n\n\n\n\n\n\nModel\nDescription\n\n\n\n\nLogistic Regression\nProvides interpretable baseline with coefficient analysis for feature importance\n\n\nRandom Forest\nOffers robust performance with built-in feature importance rankings and handling of non-linear relationships\n\n\nSupport Vector Machine\nExcels with high-dimensional feature spaces common in audio analysis\n\n\n\nEvaluation Framework: Models will be assessed using train/test split methodology with comprehensive metrics including accuracy, precision, recall, and F1-score for each emotion class, providing detailed performance analysis across emotional categories.\n\n\nQuestion 2: Ensemble Learning Effectiveness\nVariables Involved:\n\nAll audio features from Question 1 (spectral contrast, MFCCs, chroma, ZCR, RMS energy)\nindividual_predictions: Predictions from each base model (LR, RF, SVM)\nensemble_prediction: Combined prediction from ensemble methods\nindividual_confidence: Confidence scores (prediction probabilities) from each model\nensemble_confidence: Combined confidence measures from ensemble approaches\n\nAnalysis Approach:\nBase Model Training: Train all three algorithms (Logistic Regression, Random Forest, SVM) separately using identical feature sets and training data, establishing baseline performance metrics for each individual approach.\nEnsemble Creation: Implement multiple ensemble strategies including hard voting (majority vote), soft voting (probability-based), and simple probability averaging to combine individual model predictions and assess different aggregation approaches.\nPerformance Comparison: Conduct systematic comparison of individual model accuracy against ensemble methods using cross-validation, statistical significance testing, and detailed performance metrics to quantify improvement gains.\nFeature Analysis: Investigate which acoustic features contribute most effectively to each algorithm’s performance, identifying complementary strengths that ensemble methods can exploit for improved classification accuracy."
  },
  {
    "objectID": "proposal.html#technical-implementation",
    "href": "proposal.html#technical-implementation",
    "title": "Acoustic Emotion Classification",
    "section": "Technical Implementation",
    "text": "Technical Implementation\nProgramming Environment: Python with scientific computing stack\n\nKey Libraries\n\n\n\n\n\n\n\nLibrary\nPurpose\n\n\n\n\nlibrosa\nAudio feature extraction and processing\n\n\nscikit-learn\nMachine learning algorithms and evaluation\n\n\npandas / numpy\nData manipulation and numerical computation\n\n\nmatplotlib / seaborn\nVisualization and results presentation"
  },
  {
    "objectID": "proposal.html#expected-project-timeline-3-weeks",
    "href": "proposal.html#expected-project-timeline-3-weeks",
    "title": "Acoustic Emotion Classification",
    "section": "Expected Project Timeline (3~ weeks)",
    "text": "Expected Project Timeline (3~ weeks)\n\nWeek 0: Data preparation and exploration\n\nDownload CREMA-D from Kaggle (~2GB)\nExplore dataset structure and file naming\nProposal write-up and review\n\n\n\nWeek 1: Feature extraction and dataset creation\n\nImplement feature extraction pipeline\nProcess selected audio files\nCreate clean feature dataset\nExploratory data analysis of features vs emotions\n\n\n\nWeek 2: Individual model development\n\nTrain baseline models (Logistic Regression, Decision Tree, SVM)\nHyperparameter tuning using GridSearchCV\nPerformance evaluation and comparison\nFeature importance analysis\n\n\n\nWeek 3: Ensemble methods and final analysis\n\nImplement ensemble approaches\nCompare individual vs ensemble performance\nStatistical significance testing\nFinal report and presentation"
  },
  {
    "objectID": "proposal.html#project-structure",
    "href": "proposal.html#project-structure",
    "title": "Acoustic Emotion Classification",
    "section": "Project Structure",
    "text": "Project Structure\n\n\n\n\n\n\n\nFolder / File Name\nDescription\n\n\n\n\n.quarto/\nQuarto’s internal folder—automatically created to manage rendering settings and cache. You typically don’t touch this.\n\n\n_extra/\nStores supporting materials that aren’t part of the main outputs but are useful for context.\n\n\n_freeze/\nKeeps locked-in versions of documents to ensure consistency when rebuilding or sharing.\n\n\n_site/\nFinal output folder generated after rendering; includes the HTML version of your site.\n\n\ndata/\nWhere all the data lives—raw inputs, cleaned datasets, and a README explaining structure and sources.\n\n\nimages/\nUsed for storing visual content like charts, graphs, and illustrations referenced in your .qmd files.\n\n\nstyle/\nContains custom design elements, like SCSS files, that control the look and feel of your site.\n\n\nindex.qmd\nActs as the homepage, giving a snapshot of what the project is about.\n\n\nabout.qmd\nGives extra context—background info, author bio, or detailed project narrative.\n\n\nproposal.qmd\nThe full research plan: includes goals, methods, schedule, and how everything is structured.\n\n\npresentation.qmd\nSlide deck made with Quarto to highlight the most important insights from your project."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Acoustic Emotion Classification",
    "section": "",
    "text": "Add project abstract here."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Acoustic Emotion Classification",
    "section": "",
    "text": "Add project abstract here."
  },
  {
    "objectID": "presentation.html#quarto",
    "href": "presentation.html#quarto",
    "title": "Project title",
    "section": "Quarto",
    "text": "Quarto\n\nThe presentation is created using the Quarto CLI\n## sets the start of a new slide"
  },
  {
    "objectID": "presentation.html#layouts",
    "href": "presentation.html#layouts",
    "title": "Project title",
    "section": "Layouts",
    "text": "Layouts\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\nlike\nthis\n\nAnd add footnotes"
  },
  {
    "objectID": "presentation.html#code",
    "href": "presentation.html#code",
    "title": "Project title",
    "section": "Code",
    "text": "Code\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Wed, 06 Aug 2025   Prob (F-statistic):           5.84e-08\nTime:                        09:51:47   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "presentation.html#plots",
    "href": "presentation.html#plots",
    "title": "Project title",
    "section": "Plots",
    "text": "Plots"
  },
  {
    "objectID": "presentation.html#plot-and-text",
    "href": "presentation.html#plot-and-text",
    "title": "Project title",
    "section": "Plot and text",
    "text": "Plot and text\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#tables",
    "href": "presentation.html#tables",
    "title": "Project title",
    "section": "Tables",
    "text": "Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\n\n\n\nisland\n\n\n\nbill_length_mm\n\n\n\nbill_depth_mm\n\n\n\nflipper_length_mm\n\n\n\nbody_mass_g\n\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.1\n\n\n\n18.7\n\n\n\n181.0\n\n\n\n3750.0\n\n\n\nMale\n\n\n\n\n\n\n\n1\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.5\n\n\n\n17.4\n\n\n\n186.0\n\n\n\n3800.0\n\n\n\nFemale\n\n\n\n\n\n\n\n2\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n40.3\n\n\n\n18.0\n\n\n\n195.0\n\n\n\n3250.0\n\n\n\nFemale\n\n\n\n\n\n\n\n4\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n36.7\n\n\n\n19.3\n\n\n\n193.0\n\n\n\n3450.0\n\n\n\nFemale\n\n\n\n\n\n\n\n5\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.3\n\n\n\n20.6\n\n\n\n190.0\n\n\n\n3650.0\n\n\n\nMale"
  },
  {
    "objectID": "presentation.html#images",
    "href": "presentation.html#images",
    "title": "Project title",
    "section": "Images",
    "text": "Images\n\nImage credit: Danielle Navarro, Percolate."
  },
  {
    "objectID": "presentation.html#math-expressions",
    "href": "presentation.html#math-expressions",
    "title": "Project title",
    "section": "Math Expressions",
    "text": "Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $\\alpha+\\beta$ renders \\(\\alpha + \\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $."
  },
  {
    "objectID": "presentation.html#feeling-adventurous",
    "href": "presentation.html#feeling-adventurous",
    "title": "Project title",
    "section": "Feeling adventurous?",
    "text": "Feeling adventurous?\n\nYou are welcomed to use the default styling of the slides. In fact, that’s what I expect majority of you will do. You will differentiate yourself with the content of your presentation.\nBut some of you might want to play around with slide styling. Some solutions for this can be found at https://quarto.org/docs/presentations/revealjs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acoustic Emotion Classification: Optimizing Performance Through Ensemble Methods",
    "section": "",
    "text": "This project was developed by Ralph Andrade For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nRalph Andrade: 2027, MS Data Science"
  }
]