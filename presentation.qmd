---
title: "Acoustic Emotion Recognition" 
author: "Ralph Andrade"
title-slide-attributes:
  data-background-image: images/charts/abstract-acoustics.png
  data-background-size: stretch
  data-background-opacity: "0.85"
  data-slide-number: none
  class: title-slide
format:
  revealjs:
    theme:  ['style/customtheming.scss']
    css: style/custom.css
    footer: "INFO 523 Final Project | Ralph Andrade"
    transition: fade
    code-tools: true
    code-fold: true
    code-overflow: wrap
    embed-resources: true
editor: visual
jupyter: python3
execute:
  echo: false
---

```{python}
#| label: load-packages
#| include: false

# Import libs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

```

```{python}
#| label: load-data
#| include: false
# Load data in Python

df = pd.read_csv("./data/crema_d.csv", index_col=0)
```

## Abstract

-   Dataset: CREMA-D, six emotions (neutral, happy, sad, angry, fear, disgust)
-   Features: librosa, standardized, PCA (98% variance)
-   Models: SVM, Random Forest, MLP
-   MLP achieved macro F1-score: 0.5534

## Introduction

-   Automatic emotion classification is challenging
-   CREMA-D: 7,442 clips, 91 actors
-   Features: MFCCs, spectral properties
-   Goal: Compare traditional, ensemble, and neural networks

## Research Question

-   **Q1. What is the classification accuracy of unsupervised methods for emotion recognition from acoustic features?**
-   **Q2. How do standalone algorithms, ensemble approaches, and neural networks compare in terms of accuracy, robustness, and computational efficiency for emotion classification from audio?**

# Exploratory Analysis

## EDA

::: panel-tabset
### Data Overview

```{python}
#| label: dataset
#| echo: true

# number of variables and observations in the data
print(f"Total observations: {df.shape[0]}")
print(f"Number of features: {df.shape[1]}")

# missing values in each column
missing_df = df.isna().sum()
print("Numeric Description:\n", df.describe())
```

### Target Distribution

![Target Class Count Plot](images/charts/1-Target-histogram.png){fig-align="left" width="555"}
:::

## Data Preprocessing

::: panel-tabset
### Data Preparation

-   Remove irrelevant columns, handle missing values
-   Apply Yeo-Johnson Power Transformation for numeric skew
-   Encode target labels

### Skewness Transformation

![Sknewness Before & After Yeo Johnson Transformation](images/charts/2-yeo-johnson-before-after.png){fig-align="left"}
:::

## Feature engineering

::: r-fit-text
-   **Spectral Contrast:** Measures amplitude differences between spectral peaks and valleys, capturing timbral characteristics that distinguish emotional expressions
-   **MFCCs (Mel-frequency cepstral coefficients):** Extract 13 coefficients representing the short-term power spectrum, fundamental for speech emotion recognition
-   **Chroma Features:** Capture pitch class energy distribution, providing harmonic content information relevant to emotional prosody
-   **Zero-Crossing Rate:** Quantifies signal noisiness by measuring zero-axis crossings, distinguishing between voiced and unvoiced speech segments
-   **Root Mean Square (RMS) Energy:** Measures overall signal energy, correlating with loudness and emotional intensity
:::

## Principal Component Analysis

![PCA: retain 98% variance](images/charts/3-scree-plot.png){fig-align="left"}

# Model Training & Evaluation

## Model Evaulation Function

:::::: r-fit-text
::::: columns
::: {.column width="50%"}
-   Inputs:
    -   model → ML model instance (Random Forest, SVM, MLP)
    -   X_train, X_test → feature matrices
    -   y_train, y_test → labels
    -   model_name → string for labeling outputs
:::

::: {.column width="50%"}
-   Output:
    -   Console print of metrics & confusion matrix
    -   Dictionary with overall and per-class performance
:::
:::::
::::::

# Model Comparison

## Model Comparison Summary
<table border="1" cellpadding="6" cellspacing="0">
  <tr>
    <th>Model</th>
    <th>Accuracy</th>
    <th>Precision*</th>
    <th>Recall*</th>
    <th>F1-Score*</th>
  </tr>
  <tr>
    <td>RF</td>
    <td>0.5124</td>
    <td>0.5045</td>
    <td>0.5121</td>
    <td>0.5014</td>
  </tr>
  <tr>
    <td>SVM</td>
    <td>0.5285</td>
    <td>0.5258</td>
    <td>0.5280</td>
    <td>0.5256</td>
  </tr>
  <tr>
    <td>MLP</td>
    <td>0.5567</td>
    <td>0.5538</td>
    <td>0.5574</td>
    <td>0.5534</td>
  </tr>
</table>
`*` macro

## Model Metric Comparison

![Model Metric (Accuracy, Precision, Recall, F1-Score) Comparison](images/charts/7-Model-Comparision.png){fig-align="left"}

# Model Prediction & Performance

## Random Forest Overall Performance
<table border="1" cellpadding="6" cellspacing="0">
  <tr>
    <th>Metric</th>
    <th>Value</th>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>0.5104</td>
  </tr>
  <tr>
    <td>Macro Precision</td>
    <td>0.5012</td>
  </tr>
  <tr>
    <td>Macro Recall</td>
    <td>0.5104</td>
  </tr>
  <tr>
    <td>Macro F1-Score</td>
    <td>0.4990</td>
  </tr>
</table>

## Random Forest Per-Class Performance
<table border="1" cellpadding="6" cellspacing="0">
  <tr>
    <th>Emotion</th>
    <th>Precision</th>
    <th>Recall</th>
    <th>F1-Score</th>
    <th>Support</th>
  </tr>
  <tr>
    <td>angry</td>
    <td>0.5947</td>
    <td>0.7913</td>
    <td>0.6791</td>
    <td>254</td>
  </tr>
  <tr>
    <td>disgust</td>
    <td>0.4701</td>
    <td>0.4331</td>
    <td>0.4508</td>
    <td>254</td>
  </tr>
  <tr>
    <td>fear</td>
    <td>0.4615</td>
    <td>0.2835</td>
    <td>0.3512</td>
    <td>254</td>
  </tr>
  <tr>
    <td>happy</td>
    <td>0.5000</td>
    <td>0.4510</td>
    <td>0.4742</td>
    <td>255</td>
  </tr>
  <tr>
    <td>neutral</td>
    <td>0.4587</td>
    <td>0.5092</td>
    <td>0.4826</td>
    <td>218</td>
  </tr>
  <tr>
    <td>sad</td>
    <td>0.5225</td>
    <td>0.5945</td>
    <td>0.5562</td>
    <td>254</td>
  </tr>
</table>

## Random Forest Predictions

![Random Forest; Confusion Matrix](images/charts/4-RF-CM.png){fig-align="left"}

## SVM Overall Performance
<table border="1" cellpadding="6" cellspacing="0">
  <tr>
    <th>Metric</th>
    <th>Value</th>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>0.5285</td>
  </tr>
  <tr>
    <td>Macro Precision</td>
    <td>0.5258</td>
  </tr>
  <tr>
    <td>Macro Recall</td>
    <td>0.5280</td>
  </tr>
  <tr>
    <td>Macro F1-Score</td>
    <td>0.5256</td>
  </tr>
</table>

## SVM Per-Class Performance
<table border="1" cellpadding="6" cellspacing="0">
  <tr>
    <th>Emotion</th>
    <th>Precision</th>
    <th>Recall</th>
    <th>F1-Score</th>
    <th>Support</th>
  </tr>
  <tr>
    <td>angry</td>
    <td>0.6541</td>
    <td>0.7520</td>
    <td>0.6996</td>
    <td>254</td>
  </tr>
  <tr>
    <td>disgust</td>
    <td>0.4960</td>
    <td>0.4921</td>
    <td>0.4941</td>
    <td>254</td>
  </tr>
  <tr>
    <td>fear</td>
    <td>0.4478</td>
    <td>0.4724</td>
    <td>0.4598</td>
    <td>254</td>
  </tr>
  <tr>
    <td>happy</td>
    <td>0.5152</td>
    <td>0.4667</td>
    <td>0.4897</td>
    <td>255</td>
  </tr>
  <tr>
    <td>neutral</td>
    <td>0.4846</td>
    <td>0.5046</td>
    <td>0.4944</td>
    <td>218</td>
  </tr>
  <tr>
    <td>sad</td>
    <td>0.5571</td>
    <td>0.4803</td>
    <td>0.5159</td>
    <td>254</td>
  </tr>
</table>


## Support Vector Machine Predictions

![Support Vector Machine; Confusion Matrix](images/charts/5-SVM-CM.png){fig-align="left"}

## MLP (Neural Net) Overall Performance
<table border="1" cellpadding="6" cellspacing="0">
  <tr>
    <th>Metric</th>
    <th>Value</th>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>0.5567</td>
  </tr>
  <tr>
    <td>Macro Precision</td>
    <td>0.5538</td>
  </tr>
  <tr>
    <td>Macro Recall</td>
    <td>0.5574</td>
  </tr>
  <tr>
    <td>Macro F1-Score</td>
    <td>0.5534</td>
  </tr>
</table>

## MLP (Neural Net) Per-Class Performance
<table border="1" cellpadding="6" cellspacing="0">
  <tr>
    <th>Emotion</th>
    <th>Precision</th>
    <th>Recall</th>
    <th>F1-Score</th>
    <th>Support</th>
  </tr>
  <tr>
    <td>angry</td>
    <td>0.6996</td>
    <td>0.7795</td>
    <td>0.7374</td>
    <td>254</td>
  </tr>
  <tr>
    <td>disgust</td>
    <td>0.5205</td>
    <td>0.4488</td>
    <td>0.4820</td>
    <td>254</td>
  </tr>
  <tr>
    <td>fear</td>
    <td>0.4845</td>
    <td>0.4921</td>
    <td>0.4883</td>
    <td>254</td>
  </tr>
  <tr>
    <td>happy</td>
    <td>0.5463</td>
    <td>0.4627</td>
    <td>0.5011</td>
    <td>255</td>
  </tr>
  <tr>
    <td>neutral</td>
    <td>0.4792</td>
    <td>0.5826</td>
    <td>0.5259</td>
    <td>218</td>
  </tr>
  <tr>
    <td>sad</td>
    <td>0.5927</td>
    <td>0.5787</td>
    <td>0.5857</td>
    <td>254</td>
  </tr>
</table>

## Multi Layer Perceptron Predictions

![Multi Layer Perceptron; Confusion Matrix](images/charts/6-MLP-CM.png){fig-align="left"}

# Conclusion

## Key Findings
- MLP outperformed other approaches in both overall accuracy and per-emotion metrics
- Neural networks showed stronger ability to capture complex, non-linear patterns
- Delivered the most balanced performance across all classes, especially for difficult emotions (Disgust, Fear, Neutral)

## Futuer Direction
- Explore temporal modeling (RNNs, Transformers) to capture sequence information
- Integrate multi-modal inputs (audio + visual features)
- Experiment with advanced feature extraction beyond PCA
- Aim to improve classification accuracy and robustness in real-world scenarios