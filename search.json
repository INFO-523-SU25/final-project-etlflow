[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Acoustic Emotion Recognition",
    "section": "",
    "text": "This project investigates the application of machine learning techniques to classify emotions from speech audio using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset). The study focuses on distinguishing four primary emotions; happy, sad, angry, fear, neutral, and disgust, through acoustic feature analysis and explores how emotional intensity affects classification performance. By combining traditional feature engineering with ensemble learning methods, this research aims to develop robust emotion recognition models while examining the relationship between speech intensity and model confidence.\nThe project addresses two fundamental questions in acoustic emotion recognition: first, whether traditional machine learning algorithms can accurately classify emotions using engineered audio features, and second, how ensemble methods can significantly improve classification performance compared to individual models. This investigation contributes to the growing field of effective computing while demonstrating practical applications of ensemble learning principles in speech emotion recognition."
  },
  {
    "objectID": "proposal.html#project-overview",
    "href": "proposal.html#project-overview",
    "title": "Acoustic Emotion Recognition",
    "section": "",
    "text": "This project investigates the application of machine learning techniques to classify emotions from speech audio using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset). The study focuses on distinguishing four primary emotions; happy, sad, angry, fear, neutral, and disgust, through acoustic feature analysis and explores how emotional intensity affects classification performance. By combining traditional feature engineering with ensemble learning methods, this research aims to develop robust emotion recognition models while examining the relationship between speech intensity and model confidence.\nThe project addresses two fundamental questions in acoustic emotion recognition: first, whether traditional machine learning algorithms can accurately classify emotions using engineered audio features, and second, how ensemble methods can significantly improve classification performance compared to individual models. This investigation contributes to the growing field of effective computing while demonstrating practical applications of ensemble learning principles in speech emotion recognition."
  },
  {
    "objectID": "proposal.html#dataset-description",
    "href": "proposal.html#dataset-description",
    "title": "Acoustic Emotion Recognition",
    "section": "Dataset Description",
    "text": "Dataset Description\nPrimary Dataset: CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset) Provenance: CREMA-D is a validated multi modal database created through collaboration between researchers and crowd sourced validation. The dataset contains emotional speech recordings from professional actors, making it ideal for supervised learning approaches to emotion classification.\n\nDimensions & Structure:\n\n\n\n\n\n\n\nAttribute\nDetails\n\n\n\n\nTotal Files\n7,442 audio clips from diverse emotional expressions\n\n\nSpeakers\n91 professional actors (48 male, 43 female)\n\n\nAge Range\n20–74 years, providing demographic diversity\n\n\nTarget Emotions\n6 emotions selected for analysis (happy, sad, angry, fear, neutral, disgust)\n\n\nEmotional Intensities\nMultiple intensity levels (low, medium, high, unspecified)\n\n\nFile Format\nWAV files suitable for feature extraction\n\n\nSentence Variety\n12 different sentences to reduce linguistic bias\n\n\n\nDataset Selection Rationale: CREMA-D was chosen for its substantial size, demographic diversity, and established use in emotion recognition research. The dataset’s systematic organization and intensity labels directly support both research questions, while its availability through Kaggle ensures reproducible research practices."
  },
  {
    "objectID": "proposal.html#feature-engineering",
    "href": "proposal.html#feature-engineering",
    "title": "Acoustic Emotion Recognition",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nThe analysis employs five key acoustic feature categories using the librosa library:\n1. Spectral Contrast: Measures amplitude differences between spectral peaks and valleys, capturing timbral characteristics that distinguish emotional expressions\n2. MFCCs (Mel-frequency cepstral coefficients): Extract 13 coefficients representing the short-term power spectrum, fundamental for speech emotion recognition\n3. Chroma Features: Capture pitch class energy distribution, providing harmonic content information relevant to emotional prosody\n4. Zero-Crossing Rate: Quantifies signal noisiness by measuring zero-axis crossings, distinguishing between voiced and unvoiced speech segments\n5. Root Mean Square (RMS) Energy: Measures overall signal energy, correlating with loudness and emotional intensity\nThese features collectively capture the three primary acoustic dimensions of emotional expression: spectral characteristics, temporal dynamics, and energy distribution. This multi-dimensional approach aligns with established psychoacoustic research showing that human emotional perception relies on diverse auditory cues processed simultaneously."
  },
  {
    "objectID": "proposal.html#signal-processing-extract-transform-load",
    "href": "proposal.html#signal-processing-extract-transform-load",
    "title": "Acoustic Emotion Recognition",
    "section": "Signal Processing (Extract, Transform, Load)",
    "text": "Signal Processing (Extract, Transform, Load)\nThis pipeline enables the conversion of complex, high-dimensional waveforms into compact, informative representations—such as MFCCs or spectral centroids—that capture the essence of the sound.\n\n# Import libs\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport os\n\n# CREMA-D filename structure: ActorID_SentenceID_EmotionID_IntensityID.wav\naudio_dir = \"./data/CREMA_D/\"\n\ndef parse_cremad_filename(file_path):\n  name = os.path.splitext(os.path.basename(file_path))[0]\n  parts = name.split('_')\n  actor_id = parts[0]\n  sentence_id = parts[1]\n  emotion = parts[2]\n  intensity = parts[3].split('.')[0]\n  \n  # mapping emotion_id to emotions\n  emotion_map = {\"ANG\":\"angry\"\n  ,\"DIS\":\"disgust\"\n  ,\"FEA\":\"fear\"\n  ,\"HAP\":\"happy\"\n  ,\"NEU\":\"neutral\"\n  ,\"SAD\":\"sad\"}\n  \n  return {\n    \"actor_id\": actor_id\n    ,\"sentence\": sentence_id\n    ,\"emotion\": emotion_map[emotion]\n    ,\"intensity\": intensity\n  }\n\ndef extract_features(file_path):\n  \"\"\"\n    Iteratively extracts audio features from a given .wav file using librosa.\n\n    Features:\n        - Zero Crossing Rate\n        - Chroma STFT\n        - MFCCs\n        - Root Mean Square Energy\n        - Spectral Centroid\n\n    Returns:\n        list of dict: Each dictionary containing the features, and metadata parsed from the filename.\n  \"\"\"\n  # initalize empty list to store all features\n  all_features = []\n  \n  # iteratively scan os directory\n  for entry in os.scandir(file_path):\n    if entry.is_file():\n        # load .wav files iteratively from entry path\n        y, sr = librosa.load(entry.path, sr=22050)\n        \n        # initiallize empty dict\n        features = {}\n        \n        # 0. File Metadata\n        fmd = parse_cremad_filename(entry.name)\n        audio_duration = len(y)/ sr\n        features['actor_id'] = fmd['actor_id']\n        features['sentence'] = fmd['sentence']\n        features['emotion'] = fmd['emotion']\n        features['intensity'] = fmd['intensity']\n        features['audio_duration'] = audio_duration\n        features['sample_rate'] = sr\n        \n        # 1. MFCCs (most important for speech)\n        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n        for i in range(13):\n            features[f'mfcc_{i+1}_mean'] = np.mean(mfccs[i])\n            features[f'mfcc_{i+1}_std'] = np.std(mfccs[i])\n        \n        # 2. Spectral features\n        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n        features['spectral_centroid_std'] = np.std(spectral_centroid)\n        \n        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n        \n        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n        \n        # 3. Energy and rhythm\n        rms = librosa.feature.rms(y=y)\n        features['rms_mean'] = np.mean(rms)\n        features['rms_std'] = np.std(rms)\n      \n        zcr = librosa.feature.zero_crossing_rate(y)\n        features['zcr_mean'] = np.mean(zcr)\n        \n        # 4. Pitch and harmony\n        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n        features['chroma_mean'] = np.mean(chroma)\n        features['chroma_std'] = np.std(chroma)\n        \n        all_features.append(features)\n        \n  return all_features\n\n# Function call code has been commented since the data has been extracted and converted into csv file for consumption.\n# call function, while passing directory of audio files\n#audio_features = extract_features(audio_dir)\n\n# convert to a data frame\n#df = pd.DataFrame(audio_features)\n\n# export dataframe to csv for repoducibility \n#df.to_csv(\"./data/crema_d.csv\")\n\ndf = pd.read_csv(\"./data/crema_d.csv\", index_col=0)\n\n\nDataset\n\n\n\n\n\n\n\n\n\nactor_id\nsentence\nemotion\nintensity\naudio_duration\nsample_rate\nmfcc_1_mean\nmfcc_1_std\nmfcc_2_mean\nmfcc_2_std\n...\nmfcc_13_std\nspectral_centroid_mean\nspectral_centroid_std\nspectral_rolloff_mean\nspectral_bandwidth_mean\nrms_mean\nrms_std\nzcr_mean\nchroma_mean\nchroma_std\n\n\n\n\n0\n1022\nITS\nangry\nXX\n2.435782\n22050\n-266.28894\n123.935930\n108.554596\n39.694670\n...\n10.892748\n1754.476189\n965.090546\n3350.053711\n1701.814132\n0.096146\n0.110308\n0.095815\n0.328043\n0.306033\n\n\n1\n1037\nITS\nangry\nXX\n3.003039\n22050\n-346.40980\n83.344710\n125.381540\n42.769300\n...\n10.813552\n1624.501830\n1058.895061\n3325.968863\n1722.048847\n0.038797\n0.031771\n0.091384\n0.370311\n0.322135\n\n\n2\n1060\nITS\nneutral\nXX\n2.402404\n22050\n-421.48450\n36.981285\n140.371900\n19.000977\n...\n5.230837\n1406.515534\n563.173067\n3186.085862\n1848.645713\n0.008990\n0.005157\n0.060317\n0.403894\n0.303255\n\n\n3\n1075\nITS\nneutral\nXX\n2.435782\n22050\n-413.22550\n50.740425\n140.576370\n28.177567\n...\n5.935963\n1370.133893\n682.471987\n3006.958008\n1780.081965\n0.011599\n0.007132\n0.062360\n0.414367\n0.308347\n\n\n4\n1073\nIOM\ndisgust\nXX\n2.869569\n22050\n-415.93317\n61.064003\n136.759430\n19.811580\n...\n6.920853\n1164.595733\n422.881734\n2682.273028\n1686.101546\n0.014929\n0.013538\n0.041378\n0.411929\n0.306035\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n\nDataset Information\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 7442 entries, 0 to 7441\nData columns (total 41 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   actor_id                 7442 non-null   int64  \n 1   sentence                 7442 non-null   object \n 2   emotion                  7442 non-null   object \n 3   intensity                7442 non-null   object \n 4   audio_duration           7442 non-null   float64\n 5   sample_rate              7442 non-null   int64  \n 6   mfcc_1_mean              7442 non-null   float64\n 7   mfcc_1_std               7442 non-null   float64\n 8   mfcc_2_mean              7442 non-null   float64\n 9   mfcc_2_std               7442 non-null   float64\n 10  mfcc_3_mean              7442 non-null   float64\n 11  mfcc_3_std               7442 non-null   float64\n 12  mfcc_4_mean              7442 non-null   float64\n 13  mfcc_4_std               7442 non-null   float64\n 14  mfcc_5_mean              7442 non-null   float64\n 15  mfcc_5_std               7442 non-null   float64\n 16  mfcc_6_mean              7442 non-null   float64\n 17  mfcc_6_std               7442 non-null   float64\n 18  mfcc_7_mean              7442 non-null   float64\n 19  mfcc_7_std               7442 non-null   float64\n 20  mfcc_8_mean              7442 non-null   float64\n 21  mfcc_8_std               7442 non-null   float64\n 22  mfcc_9_mean              7442 non-null   float64\n 23  mfcc_9_std               7442 non-null   float64\n 24  mfcc_10_mean             7442 non-null   float64\n 25  mfcc_10_std              7442 non-null   float64\n 26  mfcc_11_mean             7442 non-null   float64\n 27  mfcc_11_std              7442 non-null   float64\n 28  mfcc_12_mean             7442 non-null   float64\n 29  mfcc_12_std              7442 non-null   float64\n 30  mfcc_13_mean             7442 non-null   float64\n 31  mfcc_13_std              7442 non-null   float64\n 32  spectral_centroid_mean   7442 non-null   float64\n 33  spectral_centroid_std    7442 non-null   float64\n 34  spectral_rolloff_mean    7442 non-null   float64\n 35  spectral_bandwidth_mean  7442 non-null   float64\n 36  rms_mean                 7442 non-null   float64\n 37  rms_std                  7442 non-null   float64\n 38  zcr_mean                 7442 non-null   float64\n 39  chroma_mean              7442 non-null   float64\n 40  chroma_std               7442 non-null   float64\ndtypes: float64(36), int64(2), object(3)\nmemory usage: 2.4+ MB\n\n\n\n\nTarget Frequency\n\n\n\n\n\n\n\n\n\n\n\nIntensity Frequency\n\n\n\n\n\n\n\n\n\n\n\nSpectral Centroid Distribution\n\n\n\n\n\n\n\n\n\n\n\nRoot Mean Square Distribution\n\n\n\n\n\n\n\n\n\n\n\nZero Crossing Rate Distribution\n\n\n\n\n\n\n\n\n\n\n\nCroma Distribution"
  },
  {
    "objectID": "proposal.html#research-questions",
    "href": "proposal.html#research-questions",
    "title": "Acoustic Emotion Recognition",
    "section": "Research Questions",
    "text": "Research Questions\n\nQuestion 1: Basic Emotion Classification\nCan we accurately classify four emotions (happy, sad, angry, fear) from audio features using traditional machine learning algorithms?\n\n\nQuestion 2: Ensemble Learning Effectiveness\nCan combining multiple machine learning algorithms (ensemble methods) significantly improve emotion classification accuracy compared to individual models, and which ensemble strategies work best for acoustic emotion recognition?"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Acoustic Emotion Recognition",
    "section": "Analysis Plan",
    "text": "Analysis Plan\n\nQuestion 1: Basic Emotion Classification\nTarget Variable: emotion\nModel Implementation: Three complementary algorithms will be implemented and compared:\n\n\n\n\n\n\n\nModel\nDescription\n\n\n\n\nRandom Forest\nOffers robust performance with built-in feature importance rankings and handling of non-linear relationships\n\n\nSupport Vector Machine\nExcels with high-dimensional feature spaces common in audio analysis\n\n\nMulti Layer Perceptron\nnotable for being able to distinguish data that is not linearly separable\n\n\n\nEvaluation Framework: Models will be assessed using train/test split methodology with comprehensive metrics including accuracy, precision, recall, and F1-score for each emotion class, providing detailed performance analysis across emotional categories.\n\n\nQuestion 2: Ensemble Learning Effectiveness\nAnalysis Approach:\nBase Model Training: Train all three algorithms (Random Forest, SVM, MLP) separately using identical feature sets and training data, establishing baseline performance metrics for each individual approach.\nPerformance Comparison: Conduct systematic comparison of individual model accuracy against ensemble methods using cross-validation, statistical significance testing, and detailed performance metrics to quantify improvement gains."
  },
  {
    "objectID": "proposal.html#technical-implementation",
    "href": "proposal.html#technical-implementation",
    "title": "Acoustic Emotion Recognition",
    "section": "Technical Implementation",
    "text": "Technical Implementation\nProgramming Environment: Python with scientific computing stack\n\nKey Libraries\n\n\n\n\n\n\n\nLibrary\nPurpose\n\n\n\n\nlibrosa\nAudio feature extraction and processing\n\n\nscikit-learn\nMachine learning algorithms and evaluation\n\n\npandas / numpy\nData manipulation and numerical computation\n\n\nmatplotlib / seaborn\nVisualization and results presentation"
  },
  {
    "objectID": "proposal.html#expected-project-timeline-3-weeks",
    "href": "proposal.html#expected-project-timeline-3-weeks",
    "title": "Acoustic Emotion Recognition",
    "section": "Expected Project Timeline (3~ weeks)",
    "text": "Expected Project Timeline (3~ weeks)\n\nWeek 0: Data preparation and exploration\n\nDownload CREMA-D from Kaggle (~2GB)\nExplore dataset structure and file naming\nProposal write-up and review\n\n\n\nWeek 1: Feature extraction and dataset creation\n\nImplement feature extraction pipeline\nProcess selected audio files\nCreate clean feature dataset\nExploratory data analysis of features vs emotions\n\n\n\nWeek 2: Individual model development\n\nTrain baseline models (Logistic Regression, Decision Tree, SVM)\nHyperparameter tuning using GridSearchCV\nPerformance evaluation and comparison\nFeature importance analysis\n\n\n\nWeek 3: Ensemble methods and final analysis\n\nImplement ensemble approaches\nCompare individual vs ensemble performance\nStatistical significance testing\nFinal report and presentation"
  },
  {
    "objectID": "proposal.html#project-structure",
    "href": "proposal.html#project-structure",
    "title": "Acoustic Emotion Recognition",
    "section": "Project Structure",
    "text": "Project Structure\n\n\n\n\n\n\n\nFolder / File Name\nDescription\n\n\n\n\n.quarto/\nQuarto’s internal folder—automatically created to manage rendering settings and cache. You typically don’t touch this.\n\n\n_extra/\nStores supporting materials that aren’t part of the main outputs but are useful for context.\n\n\n_freeze/\nKeeps locked-in versions of documents to ensure consistency when rebuilding or sharing.\n\n\ndata/\nWhere all the data lives—raw inputs, cleaned datasets, and a README explaining structure and sources.\n\n\nimages/\nUsed for storing visual content like charts, graphs, and illustrations referenced in your .qmd files.\n\n\nstyle/\nContains custom design elements, like SCSS files, that control the look and feel of your site.\n\n\nindex.qmd\nActs as the homepage, giving a snapshot of what the project is about.\n\n\nabout.qmd\nGives extra context—background info, author bio, or detailed project narrative.\n\n\nproposal.qmd\nThe full research plan: includes goals, methods, schedule, and how everything is structured.\n\n\npresentation.qmd\nSlide deck made with Quarto to highlight the most important insights from your project."
  },
  {
    "objectID": "proposal.html#work-cited",
    "href": "proposal.html#work-cited",
    "title": "Acoustic Emotion Recognition",
    "section": "Work Cited",
    "text": "Work Cited\nBanerjee, Gaurab, et al. Understanding Emotion Classification in Audio Data Stanford CS224N Custom Project.\nLivingstone, Steven R., and Frank A. Russo. “The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English.” PLOS ONE, vol. 13, no. 5, 16 May 2018, p. e0196391, www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio, https://doi.org/10.1371/journal.pone.0196391. Accessed 4 Aug. 2025.\nLok, Eu Jin. “CREMA-D.” Kaggle.com, 2019, www.kaggle.com/datasets/ejlok1/cremad. Accessed 4 Aug. 2025.\nMoataz El Ayadi, et al. “Survey on Speech Emotion Recognition: Features, Classification Schemes, and Databases.” Pattern Recognition, vol. 44, no. 3, 14 Oct. 2010, pp. 572–587, ui.adsabs.harvard.edu/abs/2011PatRe..44..572E/abstract, https://doi.org/10.1016/j.patcog.2010.09.020. Accessed 19 Aug. 2025."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Acoustic Emotion Recognition",
    "section": "",
    "text": "This study investigates the capability of machine learning models to classify emotional states from acoustic features. Using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset), we focused on six target emotions: neutral, happy, sad, angry, fear, and disgust. Numerical audio features were extracted via librosa, standardized, and reduced using PCA to retain 98% of variance. We evaluated and compared standalone algorithms (SVM), ensemble methods (Random Forest), and neural networks (MLP) for multi-class emotion recognition. Results indicate that the MLP achieved the highest macro F1-score (0.5534), demonstrating superior ability to capture non-linear patterns and balanced performance across all emotions."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Acoustic Emotion Recognition",
    "section": "",
    "text": "This study investigates the capability of machine learning models to classify emotional states from acoustic features. Using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset), we focused on six target emotions: neutral, happy, sad, angry, fear, and disgust. Numerical audio features were extracted via librosa, standardized, and reduced using PCA to retain 98% of variance. We evaluated and compared standalone algorithms (SVM), ensemble methods (Random Forest), and neural networks (MLP) for multi-class emotion recognition. Results indicate that the MLP achieved the highest macro F1-score (0.5534), demonstrating superior ability to capture non-linear patterns and balanced performance across all emotions."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Acoustic Emotion Recognition",
    "section": "Introduction",
    "text": "Introduction\nAutomatic emotion classification from speech remains a challenging problem due to the subtlety and variability of acoustic cues. This project leverages the CREMA-D dataset, comprising 7,442 .wav clips from 91 actors portraying six basic emotions. Prior research demonstrates that features such as MFCCs and spectral properties are informative for emotion detection, yet there is no consensus on optimal feature selection or model architecture (Banerjee, Huang, & Lettiere, n.d.).\nWe transformed raw audio into quantitative features using librosa and applied a standardized preprocessing pipeline. The goal is to assess the effectiveness of both traditional and neural network-based models for multi-class emotion recognition, providing a comparative evaluation of standalone algorithms, ensemble methods, and neural networks."
  },
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "Acoustic Emotion Recognition",
    "section": "Research Question",
    "text": "Research Question\n\nQ1. What is the classification accuracy of unsupervised methods for emotion recognition from acoustic features?\nQ2. How do standalone algorithms, ensemble approaches, and neural networks compare in terms of accuracy, robustness, and computational efficiency for emotion classification from audio?"
  },
  {
    "objectID": "index.html#exploratory-analysis",
    "href": "index.html#exploratory-analysis",
    "title": "Acoustic Emotion Recognition",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\nInital data load & Observations\n\n\nTotal observations: 7442\nNumber of features: 41\n          actor_id  audio_duration  sample_rate  mfcc_1_mean   mfcc_1_std  \\\ncount  7442.000000     7442.000000       7442.0  7442.000000  7442.000000   \nmean   1046.084117        2.542910      22050.0  -387.893237    81.152242   \nstd      26.243152        0.505980          0.0    56.912883    30.241790   \nmin    1001.000000        1.267982      22050.0 -1131.370700     0.000122   \n25%    1023.000000        2.202222      22050.0  -428.004615    58.292865   \n50%    1046.000000        2.502540      22050.0  -399.767640    76.243485   \n75%    1069.000000        2.836190      22050.0  -354.018697   101.771385   \nmax    1091.000000        5.005034      22050.0  -162.543350   179.528880   \n\n       mfcc_2_mean   mfcc_2_std  mfcc_3_mean   mfcc_3_std  mfcc_4_mean  ...  \\\ncount  7442.000000  7442.000000  7442.000000  7442.000000  7442.000000  ...   \nmean    131.246557    26.349166     7.226425    31.621393    50.164769  ...   \nstd      15.557340     6.193413    11.605281    11.700738    11.128262  ...   \nmin       0.000000     0.000000   -52.340374     0.000000     0.000000  ...   \n25%     122.297443    22.112123     0.674029    22.746978    42.658050  ...   \n50%     134.065410    25.892035     8.938592    30.019723    50.710929  ...   \n75%     142.583675    30.226819    15.246822    39.455707    58.216644  ...   \nmax     167.168330    63.146930    38.951794    73.551254    83.296500  ...   \n\n       mfcc_13_std  spectral_centroid_mean  spectral_centroid_std  \\\ncount  7442.000000             7442.000000            7442.000000   \nmean      6.486291             1391.389433             569.861009   \nstd       1.678969              254.030203             284.309588   \nmin       0.000000                0.000000               0.000000   \n25%       5.387114             1213.176905             356.601232   \n50%       6.221665             1335.982229             507.373775   \n75%       7.244229             1510.743971             725.217080   \nmax      24.734776             2873.927831            1699.906329   \n\n       spectral_rolloff_mean  spectral_bandwidth_mean     rms_mean  \\\ncount            7442.000000              7442.000000  7442.000000   \nmean             2959.971329              1748.984424     0.027548   \nstd               471.242990               115.947135     0.028312   \nmin                 0.000000                 0.000000     0.000000   \n25%              2648.811001              1679.011486     0.010933   \n50%              2926.667949              1748.878905     0.016707   \n75%              3211.563802              1815.428594     0.032007   \nmax              5258.158543              2163.024688     0.223023   \n\n           rms_std     zcr_mean  chroma_mean   chroma_std  \ncount  7442.000000  7442.000000  7442.000000  7442.000000  \nmean      0.027249     0.063343     0.389487     0.301066  \nstd       0.031667     0.023750     0.045327     0.012338  \nmin       0.000000     0.000000     0.000000     0.000000  \n25%       0.008040     0.047160     0.359097     0.293473  \n50%       0.015029     0.056566     0.389878     0.301533  \n75%       0.033126     0.072023     0.420108     0.309572  \nmax       0.220164     0.233774     0.553840     0.335121  \n\n[8 rows x 38 columns]\nMissing values per column:\n actor_id                   0\nsentence                   0\nemotion                    0\nintensity                  0\naudio_duration             0\nsample_rate                0\nmfcc_1_mean                0\nmfcc_1_std                 0\nmfcc_2_mean                0\nmfcc_2_std                 0\nmfcc_3_mean                0\nmfcc_3_std                 0\nmfcc_4_mean                0\nmfcc_4_std                 0\nmfcc_5_mean                0\nmfcc_5_std                 0\nmfcc_6_mean                0\nmfcc_6_std                 0\nmfcc_7_mean                0\nmfcc_7_std                 0\nmfcc_8_mean                0\nmfcc_8_std                 0\nmfcc_9_mean                0\nmfcc_9_std                 0\nmfcc_10_mean               0\nmfcc_10_std                0\nmfcc_11_mean               0\nmfcc_11_std                0\nmfcc_12_mean               0\nmfcc_12_std                0\nmfcc_13_mean               0\nmfcc_13_std                0\nspectral_centroid_mean     0\nspectral_centroid_std      0\nspectral_rolloff_mean      0\nspectral_bandwidth_mean    0\nrms_mean                   0\nrms_std                    0\nzcr_mean                   0\nchroma_mean                0\nchroma_std                 0\ndtype: int64\n\n\n\n\nTarget Class Distribution\nWe analyzed class distribution across the six emotions (neutral, happy, sad, angry, disgust, and fear). Our analysis showed that the “neutral” class has approximately 14.5% fewer samples than the other classes. Given this mild imbalance, no resampling was performed."
  },
  {
    "objectID": "index.html#data-preprocessing",
    "href": "index.html#data-preprocessing",
    "title": "Acoustic Emotion Recognition",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nData Cleaning & Transformation\nData pre-processing included handling missing values, removing irrelevant columns, and transforming numerical features to reduce skewness. The Yeo–Johnson power transformation was applied to achieve more symmetric distributions (skewness &lt; ±0.5), improving suitability for downstream modeling. The categorical target variable (‘emotion’) was encoded into numerical labels for compatibility with machine learning algorithms.\n\n\n\n\n\n\n\n\n\n\n\nSplitting the dataset\nThe data is split using the train_test_split function, with 20% of the data reserved for testing. For the features (X), we drop the target column and its encoded variant, while for the labels (y), we retain only the encoded emotion.\n\n\nData Scaling and Dimension Reduction\nBefore training our models, we applied data scaling and dimensionality reduction to the numeric features. First, we identified all numeric columns in the dataset and applied Standard Scaling to ensure each feature has zero mean and unit variance.\nNext, we performed Principal Component Analysis (PCA) to reduce dimensionality while retaining 98% of the variance. PCA transforms the scaled features into a set of orthogonal components, capturing most of the information in fewer dimensions. We evaluated the explained variance ratio for each component and visualized it using a scree plot to confirm the number of components selected."
  },
  {
    "objectID": "index.html#model-training-and-evaluation",
    "href": "index.html#model-training-and-evaluation",
    "title": "Acoustic Emotion Recognition",
    "section": "Model Training and Evaluation",
    "text": "Model Training and Evaluation\nTo address our research questions on emotion recognition from acoustic features, we trained and evaluated multiple machine learning models, including standalone algorithms, ensemble methods, and neural networks. The models were trained on the PCA-reduced and standardized feature set to improve convergence, reduce dimensionality, and mitigate potential overfitting.\nFor each model, we used the evaluate_model function, which trains the model and reports comprehensive performance metrics. These metrics include overall accuracy, macro-averaged precision, recall, and F1-score, as well as per-class performance for each emotion in the target set. To provide a visual assessment of prediction quality, confusion matrices were generated for all models.\nSpecifically, we evaluated:\n\nRandom Forest (RF): An ensemble method configured with 1000 trees, maximum depth of 15, and out-of-bag scoring to provide robust predictions while controlling for overfitting.\nSupport Vector Machine (SVM): A kernel-based model with RBF kernel, class-weight balancing for mild class imbalance, and probability estimates enabled.\nMultilayer Perceptron (MLP): A neural network with three hidden layers (128, 64, 32 neurons), early stopping, and a 10% validation split to monitor convergence.\n\n\n\nTarget emotion label mapping: {np.int64(0): 'angry', np.int64(1): 'disgust', np.int64(2): 'fear', np.int64(3): 'happy', np.int64(4): 'neutral', np.int64(5): 'sad'}\nTraining Random Forest\n==================================================\n\nRandom Forest Overall Performance:\nAccuracy: 0.5124\nMacro Precision: 0.5045\nMacro Recall: 0.5121\nMacro F1-Score: 0.5014\n\nRandom Forest Per-Class Performance:\nEmotion    Precision  Recall     F1-Score   Support   \n--------------------------------------------------\nangry      0.6067     0.7835     0.6838     254       \ndisgust    0.4744     0.4370     0.4549     254       \nfear       0.4834     0.2874     0.3605     254       \nhappy      0.4938     0.4667     0.4798     255       \nneutral    0.4519     0.4954     0.4726     218       \nsad        0.5169     0.6024     0.5564     254       \n\n\n\n\n\n\n\n\n\nTraining SVM\n==================================================\n\nSVM Overall Performance:\nAccuracy: 0.5285\nMacro Precision: 0.5258\nMacro Recall: 0.5280\nMacro F1-Score: 0.5256\n\nSVM Per-Class Performance:\nEmotion    Precision  Recall     F1-Score   Support   \n--------------------------------------------------\nangry      0.6541     0.7520     0.6996     254       \ndisgust    0.4960     0.4921     0.4941     254       \nfear       0.4478     0.4724     0.4598     254       \nhappy      0.5152     0.4667     0.4897     255       \nneutral    0.4846     0.5046     0.4944     218       \nsad        0.5571     0.4803     0.5159     254       \n\n\n\n\n\n\n\n\n\nTraining MLP(Neural Net)\n==================================================\n\nMLP(Neural Net) Overall Performance:\nAccuracy: 0.5567\nMacro Precision: 0.5538\nMacro Recall: 0.5574\nMacro F1-Score: 0.5534\n\nMLP(Neural Net) Per-Class Performance:\nEmotion    Precision  Recall     F1-Score   Support   \n--------------------------------------------------\nangry      0.6996     0.7795     0.7374     254       \ndisgust    0.5205     0.4488     0.4820     254       \nfear       0.4845     0.4921     0.4883     254       \nhappy      0.5463     0.4627     0.5011     255       \nneutral    0.4792     0.5826     0.5259     218       \nsad        0.5927     0.5787     0.5857     254"
  },
  {
    "objectID": "index.html#model-comparision",
    "href": "index.html#model-comparision",
    "title": "Acoustic Emotion Recognition",
    "section": "Model Comparision",
    "text": "Model Comparision\nWe compared the performance of three model types—Random Forest, SVM, and a multilayer perceptron (MLP), on the emotion recognition task using macro-averaged metrics and per-emotion performance. Across overall performance metrics, the MLP consistently outperformed both Random Forest and SVM, achieving the highest accuracy (0.5567), macro precision (0.5538), macro recall (0.5574), and macro F1-score (0.5534). This indicates that the neural network is most effective at capturing complex patterns in the PCA-reduced acoustic feature space.\nPer-emotion analysis revealed nuanced differences among models. For Angry, all models performed relatively well, with MLP achieving the highest F1-score (0.7374). Disgust and Fear were more challenging emotions, with lower F1-scores overall, though MLP slightly improved performance over other models. For Happy and Neutral, MLP again showed superior F1-scores, particularly in improving recall for the Neutral class (0.5826). Sad emotion classification also favored MLP, demonstrating balanced precision and recall (F1-score 0.5857).\nOverall, while ensemble methods like Random Forest and kernel-based SVM provide competitive performance for certain classes, the MLP’s ability to model non-linear interactions across multiple dimensions makes it the best-performing approach in this task. These results highlight that neural network-based models may be better suited for emotion recognition from acoustic features, addressing both classification accuracy and balanced performance across all emotions.\n\n\nMODEL COMPARISON SUMMARY\n==================================================\n             Model  Accuracy  Precision (Macro)  Recall (Macro)  \\\n0    Random Forest    0.5124             0.5045          0.5121   \n1              SVM    0.5285             0.5258          0.5280   \n2  MLP(Neural Net)    0.5567             0.5538          0.5574   \n\n   F1-Score (Macro)  \n0            0.5014  \n1            0.5256  \n2            0.5534  \n\n\n\n\n\n\n\n\n\n\nBest Performing Model: MLP(Neural Net)\nF1-Score (Macro): 0.5534\nPER-EMOTION PERFORMANCE ACROSS MODELS\n==================================================\n\nANGRY Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.6067     0.7835     0.6838    \nSVM             0.6541     0.7520     0.6996    \nMLP(Neural Net) 0.6996     0.7795     0.7374    \n\nDISGUST Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.4744     0.4370     0.4549    \nSVM             0.4960     0.4921     0.4941    \nMLP(Neural Net) 0.5205     0.4488     0.4820    \n\nFEAR Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.4834     0.2874     0.3605    \nSVM             0.4478     0.4724     0.4598    \nMLP(Neural Net) 0.4845     0.4921     0.4883    \n\nHAPPY Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.4938     0.4667     0.4798    \nSVM             0.5152     0.4667     0.4897    \nMLP(Neural Net) 0.5463     0.4627     0.5011    \n\nNEUTRAL Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.4519     0.4954     0.4726    \nSVM             0.4846     0.5046     0.4944    \nMLP(Neural Net) 0.4792     0.5826     0.5259    \n\nSAD Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.5169     0.6024     0.5564    \nSVM             0.5571     0.4803     0.5159    \nMLP(Neural Net) 0.5927     0.5787     0.5857"
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Acoustic Emotion Recognition",
    "section": "Conclusion",
    "text": "Conclusion\nThis study investigated the capability of machine learning models to classify emotional states from acoustic features using the CREMA-D dataset. Across multiple approaches including Random Forest, SVM, and a multilayer perceptron, the MLP consistently demonstrated superior performance in both overall accuracy and per-emotion metrics, highlighting its ability to capture complex nonlinear patterns in the PCA-reduced feature space. While traditional ensemble and kernel based methods provided competitive results for certain emotions, neural networks offered the most balanced performance across all classes, particularly for challenging emotions such as Disgust, Fear, and Neutral.\nThese findings suggest that for multi-class emotion recognition from audio, neural networks are better suited to leverage nuanced acoustic features compared to standalone or ensemble methods. Future work could explore integrating temporal modeling with recurrent neural networks or transformer based architectures, combining audio and visual modalities, or experimenting with advanced feature extraction methods to further improve classification accuracy and robustness."
  },
  {
    "objectID": "index.html#work-cited",
    "href": "index.html#work-cited",
    "title": "Acoustic Emotion Recognition",
    "section": "Work Cited",
    "text": "Work Cited\nBanerjee, Gaurab, et al. Understanding Emotion Classification in Audio Data Stanford CS224N Custom Project.\nLivingstone, Steven R., and Frank A. Russo. “The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English.” PLOS ONE, vol. 13, no. 5, 16 May 2018, p. e0196391, www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio, https://doi.org/10.1371/journal.pone.0196391. Accessed 4 Aug. 2025.\nLok, Eu Jin. “CREMA-D.” Kaggle.com, 2019, www.kaggle.com/datasets/ejlok1/cremad. Accessed 4 Aug. 2025.\nMoataz El Ayadi, et al. “Survey on Speech Emotion Recognition: Features, Classification Schemes, and Databases.” Pattern Recognition, vol. 44, no. 3, 14 Oct. 2010, pp. 572–587, ui.adsabs.harvard.edu/abs/2011PatRe..44..572E/abstract, https://doi.org/10.1016/j.patcog.2010.09.020. Accessed 19 Aug. 2025."
  },
  {
    "objectID": "presentation.html#abstract",
    "href": "presentation.html#abstract",
    "title": "Acoustic Emotion Recognition",
    "section": "Abstract",
    "text": "Abstract\n\nDataset: CREMA-D, six emotions (neutral, happy, sad, angry, fear, disgust)\nFeatures: librosa, standardized, PCA (98% variance)\nModels: SVM, Random Forest, MLP\nMLP achieved macro F1-score: 0.5534"
  },
  {
    "objectID": "presentation.html#introduction",
    "href": "presentation.html#introduction",
    "title": "Acoustic Emotion Recognition",
    "section": "Introduction",
    "text": "Introduction\n\nAutomatic emotion classification is challenging\nCREMA-D: 7,442 clips, 91 actors\nFeatures: MFCCs, spectral properties\nGoal: Compare traditional, ensemble, and neural networks"
  },
  {
    "objectID": "presentation.html#research-question",
    "href": "presentation.html#research-question",
    "title": "Acoustic Emotion Recognition",
    "section": "Research Question",
    "text": "Research Question\n\nQ1. What is the classification accuracy of unsupervised methods for emotion recognition from acoustic features?\nQ2. How do standalone algorithms, ensemble approaches, and neural networks compare in terms of accuracy, robustness, and computational efficiency for emotion classification from audio?"
  },
  {
    "objectID": "presentation.html#eda",
    "href": "presentation.html#eda",
    "title": "Acoustic Emotion Recognition",
    "section": "EDA",
    "text": "EDA\n\nData OverviewTarget Distribution\n\n\n\n\nCode\n# number of variables and observations in the data\nprint(f\"Total observations: {df.shape[0]}\")\nprint(f\"Number of features: {df.shape[1]}\")\n\n# missing values in each column\nmissing_df = df.isna().sum()\nprint(\"Numeric Description:\\n\", df.describe())\n\n\nTotal observations: 7442\nNumber of features: 41\nNumeric Description:\n           actor_id  audio_duration  sample_rate  mfcc_1_mean   mfcc_1_std  \\\ncount  7442.000000     7442.000000       7442.0  7442.000000  7442.000000   \nmean   1046.084117        2.542910      22050.0  -387.893237    81.152242   \nstd      26.243152        0.505980          0.0    56.912883    30.241790   \nmin    1001.000000        1.267982      22050.0 -1131.370700     0.000122   \n25%    1023.000000        2.202222      22050.0  -428.004615    58.292865   \n50%    1046.000000        2.502540      22050.0  -399.767640    76.243485   \n75%    1069.000000        2.836190      22050.0  -354.018697   101.771385   \nmax    1091.000000        5.005034      22050.0  -162.543350   179.528880   \n\n       mfcc_2_mean   mfcc_2_std  mfcc_3_mean   mfcc_3_std  mfcc_4_mean  ...  \\\ncount  7442.000000  7442.000000  7442.000000  7442.000000  7442.000000  ...   \nmean    131.246557    26.349166     7.226425    31.621393    50.164769  ...   \nstd      15.557340     6.193413    11.605281    11.700738    11.128262  ...   \nmin       0.000000     0.000000   -52.340374     0.000000     0.000000  ...   \n25%     122.297443    22.112123     0.674029    22.746978    42.658050  ...   \n50%     134.065410    25.892035     8.938592    30.019723    50.710929  ...   \n75%     142.583675    30.226819    15.246822    39.455707    58.216644  ...   \nmax     167.168330    63.146930    38.951794    73.551254    83.296500  ...   \n\n       mfcc_13_std  spectral_centroid_mean  spectral_centroid_std  \\\ncount  7442.000000             7442.000000            7442.000000   \nmean      6.486291             1391.389433             569.861009   \nstd       1.678969              254.030203             284.309588   \nmin       0.000000                0.000000               0.000000   \n25%       5.387114             1213.176905             356.601232   \n50%       6.221665             1335.982229             507.373775   \n75%       7.244229             1510.743971             725.217080   \nmax      24.734776             2873.927831            1699.906329   \n\n       spectral_rolloff_mean  spectral_bandwidth_mean     rms_mean  \\\ncount            7442.000000              7442.000000  7442.000000   \nmean             2959.971329              1748.984424     0.027548   \nstd               471.242990               115.947135     0.028312   \nmin                 0.000000                 0.000000     0.000000   \n25%              2648.811001              1679.011486     0.010933   \n50%              2926.667949              1748.878905     0.016707   \n75%              3211.563802              1815.428594     0.032007   \nmax              5258.158543              2163.024688     0.223023   \n\n           rms_std     zcr_mean  chroma_mean   chroma_std  \ncount  7442.000000  7442.000000  7442.000000  7442.000000  \nmean      0.027249     0.063343     0.389487     0.301066  \nstd       0.031667     0.023750     0.045327     0.012338  \nmin       0.000000     0.000000     0.000000     0.000000  \n25%       0.008040     0.047160     0.359097     0.293473  \n50%       0.015029     0.056566     0.389878     0.301533  \n75%       0.033126     0.072023     0.420108     0.309572  \nmax       0.220164     0.233774     0.553840     0.335121  \n\n[8 rows x 38 columns]\n\n\n\n\n\n\n\nTarget Class Count Plot"
  },
  {
    "objectID": "presentation.html#data-preprocessing",
    "href": "presentation.html#data-preprocessing",
    "title": "Acoustic Emotion Recognition",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nData PreparationSkewness Transformation\n\n\n\nRemove irrelevant columns, handle missing values\nApply Yeo-Johnson Power Transformation for numeric skew\nEncode target labels\n\n\n\n\n\n\nSknewness Before & After Yeo Johnson Transformation"
  },
  {
    "objectID": "presentation.html#feature-engineering",
    "href": "presentation.html#feature-engineering",
    "title": "Acoustic Emotion Recognition",
    "section": "Feature engineering",
    "text": "Feature engineering\n\n\nSpectral Contrast: Measures amplitude differences between spectral peaks and valleys, capturing timbral characteristics that distinguish emotional expressions\nMFCCs (Mel-frequency cepstral coefficients): Extract 13 coefficients representing the short-term power spectrum, fundamental for speech emotion recognition\nChroma Features: Capture pitch class energy distribution, providing harmonic content information relevant to emotional prosody\nZero-Crossing Rate: Quantifies signal noisiness by measuring zero-axis crossings, distinguishing between voiced and unvoiced speech segments\nRoot Mean Square (RMS) Energy: Measures overall signal energy, correlating with loudness and emotional intensity"
  },
  {
    "objectID": "presentation.html#principal-component-analysis",
    "href": "presentation.html#principal-component-analysis",
    "title": "Acoustic Emotion Recognition",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nPCA: retain 98% variance"
  },
  {
    "objectID": "presentation.html#model-evaulation-function",
    "href": "presentation.html#model-evaulation-function",
    "title": "Acoustic Emotion Recognition",
    "section": "Model Evaulation Function",
    "text": "Model Evaulation Function\n\n\n\n\nInputs:\n\nmodel → ML model instance (Random Forest, SVM, MLP)\nX_train, X_test → feature matrices\ny_train, y_test → labels\nmodel_name → string for labeling outputs\n\n\n\n\nOutput:\n\nConsole print of metrics & confusion matrix\nDictionary with overall and per-class performance"
  },
  {
    "objectID": "presentation.html#model-comparison-summary",
    "href": "presentation.html#model-comparison-summary",
    "title": "Acoustic Emotion Recognition",
    "section": "Model Comparison Summary",
    "text": "Model Comparison Summary\n\n\n\nModel\n\n\nAccuracy\n\n\nPrecision*\n\n\nRecall*\n\n\nF1-Score*\n\n\n\n\nRF\n\n\n0.5124\n\n\n0.5045\n\n\n0.5121\n\n\n0.5014\n\n\n\n\nSVM\n\n\n0.5285\n\n\n0.5258\n\n\n0.5280\n\n\n0.5256\n\n\n\n\nMLP\n\n\n0.5567\n\n\n0.5538\n\n\n0.5574\n\n\n0.5534\n\n\n\n* macro"
  },
  {
    "objectID": "presentation.html#model-metric-comparison",
    "href": "presentation.html#model-metric-comparison",
    "title": "Acoustic Emotion Recognition",
    "section": "Model Metric Comparison",
    "text": "Model Metric Comparison\n\nModel Metric (Accuracy, Precision, Recall, F1-Score) Comparison"
  },
  {
    "objectID": "presentation.html#random-forest-overall-performance",
    "href": "presentation.html#random-forest-overall-performance",
    "title": "Acoustic Emotion Recognition",
    "section": "Random Forest Overall Performance",
    "text": "Random Forest Overall Performance\n\n\n\nMetric\n\n\nValue\n\n\n\n\nAccuracy\n\n\n0.5104\n\n\n\n\nMacro Precision\n\n\n0.5012\n\n\n\n\nMacro Recall\n\n\n0.5104\n\n\n\n\nMacro F1-Score\n\n\n0.4990"
  },
  {
    "objectID": "presentation.html#random-forest-per-class-performance",
    "href": "presentation.html#random-forest-per-class-performance",
    "title": "Acoustic Emotion Recognition",
    "section": "Random Forest Per-Class Performance",
    "text": "Random Forest Per-Class Performance\n\n\n\nEmotion\n\n\nPrecision\n\n\nRecall\n\n\nF1-Score\n\n\nSupport\n\n\n\n\nangry\n\n\n0.5947\n\n\n0.7913\n\n\n0.6791\n\n\n254\n\n\n\n\ndisgust\n\n\n0.4701\n\n\n0.4331\n\n\n0.4508\n\n\n254\n\n\n\n\nfear\n\n\n0.4615\n\n\n0.2835\n\n\n0.3512\n\n\n254\n\n\n\n\nhappy\n\n\n0.5000\n\n\n0.4510\n\n\n0.4742\n\n\n255\n\n\n\n\nneutral\n\n\n0.4587\n\n\n0.5092\n\n\n0.4826\n\n\n218\n\n\n\n\nsad\n\n\n0.5225\n\n\n0.5945\n\n\n0.5562\n\n\n254"
  },
  {
    "objectID": "presentation.html#random-forest-predictions",
    "href": "presentation.html#random-forest-predictions",
    "title": "Acoustic Emotion Recognition",
    "section": "Random Forest Predictions",
    "text": "Random Forest Predictions\n\nRandom Forest; Confusion Matrix"
  },
  {
    "objectID": "presentation.html#svm-overall-performance",
    "href": "presentation.html#svm-overall-performance",
    "title": "Acoustic Emotion Recognition",
    "section": "SVM Overall Performance",
    "text": "SVM Overall Performance\n\n\n\nMetric\n\n\nValue\n\n\n\n\nAccuracy\n\n\n0.5285\n\n\n\n\nMacro Precision\n\n\n0.5258\n\n\n\n\nMacro Recall\n\n\n0.5280\n\n\n\n\nMacro F1-Score\n\n\n0.5256"
  },
  {
    "objectID": "presentation.html#svm-per-class-performance",
    "href": "presentation.html#svm-per-class-performance",
    "title": "Acoustic Emotion Recognition",
    "section": "SVM Per-Class Performance",
    "text": "SVM Per-Class Performance\n\n\n\nEmotion\n\n\nPrecision\n\n\nRecall\n\n\nF1-Score\n\n\nSupport\n\n\n\n\nangry\n\n\n0.6541\n\n\n0.7520\n\n\n0.6996\n\n\n254\n\n\n\n\ndisgust\n\n\n0.4960\n\n\n0.4921\n\n\n0.4941\n\n\n254\n\n\n\n\nfear\n\n\n0.4478\n\n\n0.4724\n\n\n0.4598\n\n\n254\n\n\n\n\nhappy\n\n\n0.5152\n\n\n0.4667\n\n\n0.4897\n\n\n255\n\n\n\n\nneutral\n\n\n0.4846\n\n\n0.5046\n\n\n0.4944\n\n\n218\n\n\n\n\nsad\n\n\n0.5571\n\n\n0.4803\n\n\n0.5159\n\n\n254"
  },
  {
    "objectID": "presentation.html#support-vector-machine-predictions",
    "href": "presentation.html#support-vector-machine-predictions",
    "title": "Acoustic Emotion Recognition",
    "section": "Support Vector Machine Predictions",
    "text": "Support Vector Machine Predictions\n\nSupport Vector Machine; Confusion Matrix"
  },
  {
    "objectID": "presentation.html#mlp-neural-net-overall-performance",
    "href": "presentation.html#mlp-neural-net-overall-performance",
    "title": "Acoustic Emotion Recognition",
    "section": "MLP (Neural Net) Overall Performance",
    "text": "MLP (Neural Net) Overall Performance\n\n\n\nMetric\n\n\nValue\n\n\n\n\nAccuracy\n\n\n0.5567\n\n\n\n\nMacro Precision\n\n\n0.5538\n\n\n\n\nMacro Recall\n\n\n0.5574\n\n\n\n\nMacro F1-Score\n\n\n0.5534"
  },
  {
    "objectID": "presentation.html#mlp-neural-net-per-class-performance",
    "href": "presentation.html#mlp-neural-net-per-class-performance",
    "title": "Acoustic Emotion Recognition",
    "section": "MLP (Neural Net) Per-Class Performance",
    "text": "MLP (Neural Net) Per-Class Performance\n\n\n\nEmotion\n\n\nPrecision\n\n\nRecall\n\n\nF1-Score\n\n\nSupport\n\n\n\n\nangry\n\n\n0.6996\n\n\n0.7795\n\n\n0.7374\n\n\n254\n\n\n\n\ndisgust\n\n\n0.5205\n\n\n0.4488\n\n\n0.4820\n\n\n254\n\n\n\n\nfear\n\n\n0.4845\n\n\n0.4921\n\n\n0.4883\n\n\n254\n\n\n\n\nhappy\n\n\n0.5463\n\n\n0.4627\n\n\n0.5011\n\n\n255\n\n\n\n\nneutral\n\n\n0.4792\n\n\n0.5826\n\n\n0.5259\n\n\n218\n\n\n\n\nsad\n\n\n0.5927\n\n\n0.5787\n\n\n0.5857\n\n\n254"
  },
  {
    "objectID": "presentation.html#multi-layer-perceptron-predictions",
    "href": "presentation.html#multi-layer-perceptron-predictions",
    "title": "Acoustic Emotion Recognition",
    "section": "Multi Layer Perceptron Predictions",
    "text": "Multi Layer Perceptron Predictions\n\nMulti Layer Perceptron; Confusion Matrix"
  },
  {
    "objectID": "presentation.html#key-findings",
    "href": "presentation.html#key-findings",
    "title": "Acoustic Emotion Recognition",
    "section": "Key Findings",
    "text": "Key Findings\n\nMLP outperformed other approaches in both overall accuracy and per-emotion metrics\nNeural networks showed stronger ability to capture complex, non-linear patterns\nDelivered the most balanced performance across all classes, especially for difficult emotions (Disgust, Fear, Neutral)"
  },
  {
    "objectID": "presentation.html#futuer-direction",
    "href": "presentation.html#futuer-direction",
    "title": "Acoustic Emotion Recognition",
    "section": "Futuer Direction",
    "text": "Futuer Direction\n\nExplore temporal modeling (RNNs, Transformers) to capture sequence information\nIntegrate multi-modal inputs (audio + visual features)\nExperiment with advanced feature extraction beyond PCA\nAim to improve classification accuracy and robustness in real-world scenarios"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acoustic Emotion Recognition",
    "section": "",
    "text": "This project was developed by Ralph Andrade For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism."
  }
]