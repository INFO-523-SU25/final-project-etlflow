[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Acoustic Emotion Classification",
    "section": "",
    "text": "This project investigates the application of machine learning techniques to classify emotions from speech audio using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset). The study focuses on distinguishing four primary emotions—happy, sad, angry, and fear—through acoustic feature analysis and explores how emotional intensity affects classification performance. By combining traditional feature engineering with ensemble learning methods, this research aims to develop robust emotion recognition models while examining the relationship between speech intensity and model confidence.\nThe project addresses two fundamental questions in acoustic emotion recognition: first, whether traditional machine learning algorithms can accurately classify emotions using engineered audio features, and second, how ensemble methods can significantly improve classification performance compared to individual models. This investigation contributes to the growing field of effective computing while demonstrating practical applications of ensemble learning principles in speech emotion recognition."
  },
  {
    "objectID": "proposal.html#project-overview",
    "href": "proposal.html#project-overview",
    "title": "Acoustic Emotion Classification",
    "section": "",
    "text": "This project investigates the application of machine learning techniques to classify emotions from speech audio using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset). The study focuses on distinguishing four primary emotions—happy, sad, angry, and fear—through acoustic feature analysis and explores how emotional intensity affects classification performance. By combining traditional feature engineering with ensemble learning methods, this research aims to develop robust emotion recognition models while examining the relationship between speech intensity and model confidence.\nThe project addresses two fundamental questions in acoustic emotion recognition: first, whether traditional machine learning algorithms can accurately classify emotions using engineered audio features, and second, how ensemble methods can significantly improve classification performance compared to individual models. This investigation contributes to the growing field of effective computing while demonstrating practical applications of ensemble learning principles in speech emotion recognition."
  },
  {
    "objectID": "proposal.html#dataset-description",
    "href": "proposal.html#dataset-description",
    "title": "Acoustic Emotion Classification",
    "section": "Dataset Description",
    "text": "Dataset Description\nPrimary Dataset: CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset) Provenance: CREMA-D is a validated multi modal database created through collaboration between researchers and crowd sourced validation. The dataset contains emotional speech recordings from professional actors, making it ideal for supervised learning approaches to emotion classification.\n\nDimensions & Structure:\n\n\n\n\n\n\n\nAttribute\nDetails\n\n\n\n\nTotal Files\n7,442 audio clips from diverse emotional expressions\n\n\nSpeakers\n91 professional actors (48 male, 43 female)\n\n\nAge Range\n20–74 years, providing demographic diversity\n\n\nTarget Emotions\n4 emotions selected for analysis (happy, sad, angry, fear)\n\n\nEmotional Intensities\nMultiple intensity levels (low, medium, high, unspecified)\n\n\nFile Format\nWAV files suitable for feature extraction\n\n\nSentence Variety\n12 different sentences to reduce linguistic bias\n\n\n\nDataset Selection Rationale: CREMA-D was chosen for its substantial size, demographic diversity, and established use in emotion recognition research. The dataset’s systematic organization and intensity labels directly support both research questions, while its availability through Kaggle ensures reproducible research practices."
  },
  {
    "objectID": "proposal.html#signal-processing-extract-transform-load",
    "href": "proposal.html#signal-processing-extract-transform-load",
    "title": "Acoustic Emotion Classification",
    "section": "Signal Processing (Extract, Transform, Load)",
    "text": "Signal Processing (Extract, Transform, Load)\nThis pipeline enables the conversion of complex, high-dimensional waveforms into compact, informative representations—such as MFCCs or spectral centroids—that capture the essence of the sound.\n\n# Import libs\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport os\n\n# CREMA-D filename structure: ActorID_SentenceID_EmotionID_IntensityID.wav\naudio_dir = \"./data/CREMA_D/\"\n\ndef parse_cremad_filename(file_path):\n  name = os.path.splitext(os.path.basename(file_path))[0]\n  parts = name.split('_')\n  actor_id = parts[0]\n  sentence_id = parts[1]\n  emotion = parts[2]\n  intensity = parts[3].split('.')[0]\n  \n  # mapping emotion_id to emotions\n  emotion_map = {\"ANG\":\"angry\"\n  ,\"DIS\":\"disgust\"\n  ,\"FEA\":\"fear\"\n  ,\"HAP\":\"happy\"\n  ,\"NEU\":\"neutral\"\n  ,\"SAD\":\"sad\"}\n  \n  return {\n    \"actor_id\": actor_id\n    ,\"sentence\": sentence_id\n    ,\"emotion\": emotion_map[emotion]\n    ,\"intensity\": intensity\n  }\n\ndef extract_features(file_path):\n  \"\"\"\n    Iteratively extracts audio features from a given .wav file using librosa.\n\n    Features:\n        - Zero Crossing Rate\n        - Chroma STFT\n        - MFCCs\n        - Root Mean Square Energy\n        - Spectral Centroid\n\n    Returns:\n        list of dict: Each dictionary containing the features, and metadata parsed from the filename.\n  \"\"\"\n  # initalize empty list to store all features\n  all_features = []\n  \n  # iteratively scan os directory\n  for entry in os.scandir(file_path):\n    if entry.is_file():\n        # load .wav files iteratively from entry path\n        y, sr = librosa.load(entry.path, sr=22050)\n        \n        # initiallize empty dict\n        features = {}\n        \n        # 0. File Metadata\n        fmd = parse_cremad_filename(entry.name)\n        audio_duration = len(y)/ sr\n        features['actor_id'] = fmd['actor_id']\n        features['sentence'] = fmd['sentence']\n        features['emotion'] = fmd['emotion']\n        features['intensity'] = fmd['intensity']\n        features['audio_duration'] = audio_duration\n        features['sample_rate'] = sr\n        \n        # 1. MFCCs (most important for speech)\n        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n        for i in range(13):\n            features[f'mfcc_{i+1}_mean'] = np.mean(mfccs[i])\n            features[f'mfcc_{i+1}_std'] = np.std(mfccs[i])\n        \n        # 2. Spectral features\n        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n        features['spectral_centroid_std'] = np.std(spectral_centroid)\n        \n        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n        \n        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n        \n        # 3. Energy and rhythm\n        rms = librosa.feature.rms(y=y)\n        features['rms_mean'] = np.mean(rms)\n        features['rms_std'] = np.std(rms)\n      \n        zcr = librosa.feature.zero_crossing_rate(y)\n        features['zcr_mean'] = np.mean(zcr)\n        \n        # 4. Pitch and harmony\n        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n        features['chroma_mean'] = np.mean(chroma)\n        features['chroma_std'] = np.std(chroma)\n        \n        all_features.append(features)\n        \n  return all_features\n\n# Function call code has been commented since the data has been extracted and converted into csv file for consumption.\n# call function, while passing directory of audio files\n#audio_features = extract_features(audio_dir)\n\n# convert to a data frame\n#df = pd.DataFrame(audio_features)\n\n# export dataframe to csv for repoducibility \n#df.to_csv(\"./data/crema_d.csv\")\n\ndf = pd.read_csv(\"./data/crema_d.csv\", index_col=0)\n\n\nDataset\n\n\n\n\n\n\n\n\n\nactor_id\nsentence\nemotion\nintensity\naudio_duration\nsample_rate\nmfcc_1_mean\nmfcc_1_std\nmfcc_2_mean\nmfcc_2_std\n...\nmfcc_13_std\nspectral_centroid_mean\nspectral_centroid_std\nspectral_rolloff_mean\nspectral_bandwidth_mean\nrms_mean\nrms_std\nzcr_mean\nchroma_mean\nchroma_std\n\n\n\n\n0\n1022\nITS\nangry\nXX\n2.435782\n22050\n-266.28894\n123.935930\n108.554596\n39.694670\n...\n10.892748\n1754.476189\n965.090546\n3350.053711\n1701.814132\n0.096146\n0.110308\n0.095815\n0.328043\n0.306033\n\n\n1\n1037\nITS\nangry\nXX\n3.003039\n22050\n-346.40980\n83.344710\n125.381540\n42.769300\n...\n10.813552\n1624.501830\n1058.895061\n3325.968863\n1722.048847\n0.038797\n0.031771\n0.091384\n0.370311\n0.322135\n\n\n2\n1060\nITS\nneutral\nXX\n2.402404\n22050\n-421.48450\n36.981285\n140.371900\n19.000977\n...\n5.230837\n1406.515534\n563.173067\n3186.085862\n1848.645713\n0.008990\n0.005157\n0.060317\n0.403894\n0.303255\n\n\n3\n1075\nITS\nneutral\nXX\n2.435782\n22050\n-413.22550\n50.740425\n140.576370\n28.177567\n...\n5.935963\n1370.133893\n682.471987\n3006.958008\n1780.081965\n0.011599\n0.007132\n0.062360\n0.414367\n0.308347\n\n\n4\n1073\nIOM\ndisgust\nXX\n2.869569\n22050\n-415.93317\n61.064003\n136.759430\n19.811580\n...\n6.920853\n1164.595733\n422.881734\n2682.273028\n1686.101546\n0.014929\n0.013538\n0.041378\n0.411929\n0.306035\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n\nDataset Information\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 7442 entries, 0 to 7441\nData columns (total 41 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   actor_id                 7442 non-null   int64  \n 1   sentence                 7442 non-null   object \n 2   emotion                  7442 non-null   object \n 3   intensity                7442 non-null   object \n 4   audio_duration           7442 non-null   float64\n 5   sample_rate              7442 non-null   int64  \n 6   mfcc_1_mean              7442 non-null   float64\n 7   mfcc_1_std               7442 non-null   float64\n 8   mfcc_2_mean              7442 non-null   float64\n 9   mfcc_2_std               7442 non-null   float64\n 10  mfcc_3_mean              7442 non-null   float64\n 11  mfcc_3_std               7442 non-null   float64\n 12  mfcc_4_mean              7442 non-null   float64\n 13  mfcc_4_std               7442 non-null   float64\n 14  mfcc_5_mean              7442 non-null   float64\n 15  mfcc_5_std               7442 non-null   float64\n 16  mfcc_6_mean              7442 non-null   float64\n 17  mfcc_6_std               7442 non-null   float64\n 18  mfcc_7_mean              7442 non-null   float64\n 19  mfcc_7_std               7442 non-null   float64\n 20  mfcc_8_mean              7442 non-null   float64\n 21  mfcc_8_std               7442 non-null   float64\n 22  mfcc_9_mean              7442 non-null   float64\n 23  mfcc_9_std               7442 non-null   float64\n 24  mfcc_10_mean             7442 non-null   float64\n 25  mfcc_10_std              7442 non-null   float64\n 26  mfcc_11_mean             7442 non-null   float64\n 27  mfcc_11_std              7442 non-null   float64\n 28  mfcc_12_mean             7442 non-null   float64\n 29  mfcc_12_std              7442 non-null   float64\n 30  mfcc_13_mean             7442 non-null   float64\n 31  mfcc_13_std              7442 non-null   float64\n 32  spectral_centroid_mean   7442 non-null   float64\n 33  spectral_centroid_std    7442 non-null   float64\n 34  spectral_rolloff_mean    7442 non-null   float64\n 35  spectral_bandwidth_mean  7442 non-null   float64\n 36  rms_mean                 7442 non-null   float64\n 37  rms_std                  7442 non-null   float64\n 38  zcr_mean                 7442 non-null   float64\n 39  chroma_mean              7442 non-null   float64\n 40  chroma_std               7442 non-null   float64\ndtypes: float64(36), int64(2), object(3)\nmemory usage: 2.4+ MB\n\n\n\n\nTarget Frequency\n\n\n\n\n\n\n\n\n\n\n\nIntensity Frequency\n\n\n\n\n\n\n\n\n\n\n\nSpectral Centroid Distribution\n\n\n\n\n\n\n\n\n\n\n\nRoot Mean Square Distribution\n\n\n\n\n\n\n\n\n\n\n\nZero Crossing Rate Distribution\n\n\n\n\n\n\n\n\n\n\n\nCroma Distribution"
  },
  {
    "objectID": "proposal.html#research-questions",
    "href": "proposal.html#research-questions",
    "title": "Acoustic Emotion Classification",
    "section": "Research Questions",
    "text": "Research Questions\n\nQuestion 1: Basic Emotion Classification\nCan we accurately classify four emotions (happy, sad, angry, fear) from audio features using traditional machine learning algorithms? This question investigates the fundamental capability of machine learning models to distinguish emotional states through acoustic analysis. By focusing on four distinct emotions, the study maintains sufficient complexity while ensuring manageable scope for comprehensive analysis.\n\n\nQuestion 2: Ensemble Learning Effectiveness\nCan combining multiple machine learning algorithms (ensemble methods) significantly improve emotion classification accuracy compared to individual models, and which ensemble strategies work best for acoustic emotion recognition? This question explores the relationship between emotional expression intensity and model performance, investigating whether stronger emotional expressions are easier to classify and whether audio features can predict when models will be confident in their predictions."
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Acoustic Emotion Classification",
    "section": "Analysis Plan",
    "text": "Analysis Plan\n\nQuestion 1: Basic Emotion Classification\nTarget Variable: emotion (4-class: happy, sad, angry, fear)\nFeature Extraction Strategy: The analysis employs five key acoustic feature categories using the librosa library:\n1. Spectral Contrast: Measures amplitude differences between spectral peaks and valleys, capturing timbral characteristics that distinguish emotional expressions\n2. MFCCs (Mel-frequency cepstral coefficients): Extract 13 coefficients representing the short-term power spectrum, fundamental for speech emotion recognition\n3. Chroma Features: Capture pitch class energy distribution, providing harmonic content information relevant to emotional prosody\n4. Zero-Crossing Rate: Quantifies signal noisiness by measuring zero-axis crossings, distinguishing between voiced and unvoiced speech segments\n5. Root Mean Square (RMS) Energy: Measures overall signal energy, correlating with loudness and emotional intensity\nModel Implementation: Three complementary algorithms will be implemented and compared:\n\n\n\n\n\n\n\nModel\nDescription\n\n\n\n\nLogistic Regression\nProvides interpretable baseline with coefficient analysis for feature importance\n\n\nRandom Forest\nOffers robust performance with built-in feature importance rankings and handling of non-linear relationships\n\n\nSupport Vector Machine\nExcels with high-dimensional feature spaces common in audio analysis\n\n\n\nEvaluation Framework: Models will be assessed using train/test split methodology with comprehensive metrics including accuracy, precision, recall, and F1-score for each emotion class, providing detailed performance analysis across emotional categories.\n\n\nQuestion 2: Ensemble Learning Effectiveness\nVariables Involved:\n\nAll audio features from Question 1 (spectral contrast, MFCCs, chroma, ZCR, RMS energy)\nindividual_predictions: Predictions from each base model (LR, RF, SVM)\nensemble_prediction: Combined prediction from ensemble methods\nindividual_confidence: Confidence scores (prediction probabilities) from each model\nensemble_confidence: Combined confidence measures from ensemble approaches\n\nAnalysis Approach:\nBase Model Training: Train all three algorithms (Logistic Regression, Random Forest, SVM) separately using identical feature sets and training data, establishing baseline performance metrics for each individual approach.\nEnsemble Creation: Implement multiple ensemble strategies including hard voting (majority vote), soft voting (probability-based), and simple probability averaging to combine individual model predictions and assess different aggregation approaches.\nPerformance Comparison: Conduct systematic comparison of individual model accuracy against ensemble methods using cross-validation, statistical significance testing, and detailed performance metrics to quantify improvement gains.\nFeature Analysis: Investigate which acoustic features contribute most effectively to each algorithm’s performance, identifying complementary strengths that ensemble methods can exploit for improved classification accuracy."
  },
  {
    "objectID": "proposal.html#technical-implementation",
    "href": "proposal.html#technical-implementation",
    "title": "Acoustic Emotion Classification",
    "section": "Technical Implementation",
    "text": "Technical Implementation\nProgramming Environment: Python with scientific computing stack\n\nKey Libraries\n\n\n\n\n\n\n\nLibrary\nPurpose\n\n\n\n\nlibrosa\nAudio feature extraction and processing\n\n\nscikit-learn\nMachine learning algorithms and evaluation\n\n\npandas / numpy\nData manipulation and numerical computation\n\n\nmatplotlib / seaborn\nVisualization and results presentation"
  },
  {
    "objectID": "proposal.html#expected-project-timeline-3-weeks",
    "href": "proposal.html#expected-project-timeline-3-weeks",
    "title": "Acoustic Emotion Classification",
    "section": "Expected Project Timeline (3~ weeks)",
    "text": "Expected Project Timeline (3~ weeks)\n\nWeek 0: Data preparation and exploration\n\nDownload CREMA-D from Kaggle (~2GB)\nExplore dataset structure and file naming\nProposal write-up and review\n\n\n\nWeek 1: Feature extraction and dataset creation\n\nImplement feature extraction pipeline\nProcess selected audio files\nCreate clean feature dataset\nExploratory data analysis of features vs emotions\n\n\n\nWeek 2: Individual model development\n\nTrain baseline models (Logistic Regression, Decision Tree, SVM)\nHyperparameter tuning using GridSearchCV\nPerformance evaluation and comparison\nFeature importance analysis\n\n\n\nWeek 3: Ensemble methods and final analysis\n\nImplement ensemble approaches\nCompare individual vs ensemble performance\nStatistical significance testing\nFinal report and presentation"
  },
  {
    "objectID": "proposal.html#project-structure",
    "href": "proposal.html#project-structure",
    "title": "Acoustic Emotion Classification",
    "section": "Project Structure",
    "text": "Project Structure\n\n\n\n\n\n\n\nFolder / File Name\nDescription\n\n\n\n\n.quarto/\nQuarto’s internal folder—automatically created to manage rendering settings and cache. You typically don’t touch this.\n\n\n_extra/\nStores supporting materials that aren’t part of the main outputs but are useful for context.\n\n\n_freeze/\nKeeps locked-in versions of documents to ensure consistency when rebuilding or sharing.\n\n\n_site/\nFinal output folder generated after rendering; includes the HTML version of your site.\n\n\ndata/\nWhere all the data lives—raw inputs, cleaned datasets, and a README explaining structure and sources.\n\n\nimages/\nUsed for storing visual content like charts, graphs, and illustrations referenced in your .qmd files.\n\n\nstyle/\nContains custom design elements, like SCSS files, that control the look and feel of your site.\n\n\nindex.qmd\nActs as the homepage, giving a snapshot of what the project is about.\n\n\nabout.qmd\nGives extra context—background info, author bio, or detailed project narrative.\n\n\nproposal.qmd\nThe full research plan: includes goals, methods, schedule, and how everything is structured.\n\n\npresentation.qmd\nSlide deck made with Quarto to highlight the most important insights from your project."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Acoustic Emotion Classification",
    "section": "",
    "text": "This study investigates the capability of machine learning models to classify emotional states from acoustic features. Using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset), we focused on six target emotions: neutral, happy, sad, angry, fear, and disgust. Numerical audio features were extracted via librosa, standardized, and reduced using PCA to retain 98% of variance. We evaluated and compared standalone algorithms (SVM), ensemble methods (Random Forest), and neural networks (MLP) for multi-class emotion recognition. Results indicate that the MLP achieved the highest macro F1-score (0.5534), demonstrating superior ability to capture non-linear patterns and balanced performance across all emotions."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Acoustic Emotion Classification",
    "section": "",
    "text": "This study investigates the capability of machine learning models to classify emotional states from acoustic features. Using the CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset), we focused on six target emotions: neutral, happy, sad, angry, fear, and disgust. Numerical audio features were extracted via librosa, standardized, and reduced using PCA to retain 98% of variance. We evaluated and compared standalone algorithms (SVM), ensemble methods (Random Forest), and neural networks (MLP) for multi-class emotion recognition. Results indicate that the MLP achieved the highest macro F1-score (0.5534), demonstrating superior ability to capture non-linear patterns and balanced performance across all emotions."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Acoustic Emotion Classification",
    "section": "Introduction",
    "text": "Introduction\nAutomatic emotion classification from speech remains a challenging problem due to the subtlety and variability of acoustic cues. This project leverages the CREMA-D dataset, comprising 7,442 .wav clips from 91 actors portraying six basic emotions. Prior research demonstrates that features such as MFCCs and spectral properties are informative for emotion detection, yet there is no consensus on optimal feature selection or model architecture (Banerjee, Huang, & Lettiere, n.d.).\nWe transformed raw audio into quantitative features using librosa and applied a standardized preprocessing pipeline. The goal is to assess the effectiveness of both traditional and neural network-based models for multi-class emotion recognition, providing a comparative evaluation of standalone algorithms, ensemble methods, and neural networks."
  },
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "Acoustic Emotion Classification",
    "section": "Research Question",
    "text": "Research Question\n\nQ1. What is the classification accuracy of unsupervised methods for emotion recognition from acoustic features?\nQ2. How do standalone algorithms, ensemble approaches, and neural networks compare in terms of accuracy, robustness, and computational efficiency for emotion classification from audio?"
  },
  {
    "objectID": "index.html#exploratory-analysis",
    "href": "index.html#exploratory-analysis",
    "title": "Acoustic Emotion Classification",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\nInital data load & Observations\n\n# Import libs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import PowerTransformer, LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\n\n# Import required libraries for models and metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import (classification_report, confusion_matrix, \n                           accuracy_score, precision_score, recall_score, f1_score)\n\n#import data into df\ndf = pd.read_csv(\"./data/crema_d.csv\", index_col=0)\n\n# number of variables and observations in the data\nprint(f\"Total observations: {df.shape[0]}\")\nprint(f\"Number of features: {df.shape[1]}\")\n\n# Numeric summary\nprint(df.describe())\n\n# missing values in each column\nmissing_df = df.isna().sum()\nprint(\"Missing values per column:\\n\", missing_df)\n\nTotal observations: 7442\nNumber of features: 41\n          actor_id  audio_duration  sample_rate  mfcc_1_mean   mfcc_1_std  \\\ncount  7442.000000     7442.000000       7442.0  7442.000000  7442.000000   \nmean   1046.084117        2.542910      22050.0  -387.893237    81.152242   \nstd      26.243152        0.505980          0.0    56.912883    30.241790   \nmin    1001.000000        1.267982      22050.0 -1131.370700     0.000122   \n25%    1023.000000        2.202222      22050.0  -428.004615    58.292865   \n50%    1046.000000        2.502540      22050.0  -399.767640    76.243485   \n75%    1069.000000        2.836190      22050.0  -354.018697   101.771385   \nmax    1091.000000        5.005034      22050.0  -162.543350   179.528880   \n\n       mfcc_2_mean   mfcc_2_std  mfcc_3_mean   mfcc_3_std  mfcc_4_mean  ...  \\\ncount  7442.000000  7442.000000  7442.000000  7442.000000  7442.000000  ...   \nmean    131.246557    26.349166     7.226425    31.621393    50.164769  ...   \nstd      15.557340     6.193413    11.605281    11.700738    11.128262  ...   \nmin       0.000000     0.000000   -52.340374     0.000000     0.000000  ...   \n25%     122.297443    22.112123     0.674029    22.746978    42.658050  ...   \n50%     134.065410    25.892035     8.938592    30.019723    50.710929  ...   \n75%     142.583675    30.226819    15.246822    39.455707    58.216644  ...   \nmax     167.168330    63.146930    38.951794    73.551254    83.296500  ...   \n\n       mfcc_13_std  spectral_centroid_mean  spectral_centroid_std  \\\ncount  7442.000000             7442.000000            7442.000000   \nmean      6.486291             1391.389433             569.861009   \nstd       1.678969              254.030203             284.309588   \nmin       0.000000                0.000000               0.000000   \n25%       5.387114             1213.176905             356.601232   \n50%       6.221665             1335.982229             507.373775   \n75%       7.244229             1510.743971             725.217080   \nmax      24.734776             2873.927831            1699.906329   \n\n       spectral_rolloff_mean  spectral_bandwidth_mean     rms_mean  \\\ncount            7442.000000              7442.000000  7442.000000   \nmean             2959.971329              1748.984424     0.027548   \nstd               471.242990               115.947135     0.028312   \nmin                 0.000000                 0.000000     0.000000   \n25%              2648.811001              1679.011486     0.010933   \n50%              2926.667949              1748.878905     0.016707   \n75%              3211.563802              1815.428594     0.032007   \nmax              5258.158543              2163.024688     0.223023   \n\n           rms_std     zcr_mean  chroma_mean   chroma_std  \ncount  7442.000000  7442.000000  7442.000000  7442.000000  \nmean      0.027249     0.063343     0.389487     0.301066  \nstd       0.031667     0.023750     0.045327     0.012338  \nmin       0.000000     0.000000     0.000000     0.000000  \n25%       0.008040     0.047160     0.359097     0.293473  \n50%       0.015029     0.056566     0.389878     0.301533  \n75%       0.033126     0.072023     0.420108     0.309572  \nmax       0.220164     0.233774     0.553840     0.335121  \n\n[8 rows x 38 columns]\nMissing values per column:\n actor_id                   0\nsentence                   0\nemotion                    0\nintensity                  0\naudio_duration             0\nsample_rate                0\nmfcc_1_mean                0\nmfcc_1_std                 0\nmfcc_2_mean                0\nmfcc_2_std                 0\nmfcc_3_mean                0\nmfcc_3_std                 0\nmfcc_4_mean                0\nmfcc_4_std                 0\nmfcc_5_mean                0\nmfcc_5_std                 0\nmfcc_6_mean                0\nmfcc_6_std                 0\nmfcc_7_mean                0\nmfcc_7_std                 0\nmfcc_8_mean                0\nmfcc_8_std                 0\nmfcc_9_mean                0\nmfcc_9_std                 0\nmfcc_10_mean               0\nmfcc_10_std                0\nmfcc_11_mean               0\nmfcc_11_std                0\nmfcc_12_mean               0\nmfcc_12_std                0\nmfcc_13_mean               0\nmfcc_13_std                0\nspectral_centroid_mean     0\nspectral_centroid_std      0\nspectral_rolloff_mean      0\nspectral_bandwidth_mean    0\nrms_mean                   0\nrms_std                    0\nzcr_mean                   0\nchroma_mean                0\nchroma_std                 0\ndtype: int64\n\n\n\n\nTarget Class Distribution\nWe analyzed class distribution across the six emotions (neutral, happy, sad, angry, disgust, and fear). Our analysis showed that the “neutral” class has approximately 14.5% fewer samples than the other classes. Given this mild imbalance, no resampling was performed.\n\nplt.figure(figsize=(8, 6))\nsns.countplot(data=df, x='emotion', hue='emotion')\nplt.title(\"Target Counts\")\nplt.xticks(rotation=45)\nplt.show()\n\n# Show class distribution for all emotions\nemotion_counts = df['emotion'].value_counts()\n#print(\"Class distribution:\\n\", emotion_counts)\n\n# Focus on the four target emotions\ntarget_emotions = ['happy', 'sad', 'angry', 'fear', 'neutral','disgust']\ntarget_counts = df[df['emotion'].isin(target_emotions)]['emotion'].value_counts()\n#print(\"\\nTarget emotion counts:\\n\", target_counts)"
  },
  {
    "objectID": "index.html#data-preprocessing",
    "href": "index.html#data-preprocessing",
    "title": "Acoustic Emotion Classification",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nData Cleaning & Transformation\nData pre-processing included handling missing values, removing irrelevant columns, and transforming numerical features to reduce skewness. The Yeo–Johnson power transformation was applied to achieve more symmetric distributions (skewness &lt; ±0.5), improving suitability for downstream modeling. The categorical target variable (‘emotion’) was encoded into numerical labels for compatibility with machine learning algorithms.\n\n# Remove unwanted columns\ndf = df.drop(columns=['actor_id', 'sentence', 'intensity','sample_rate'])\n\n#Indentify skew of numeric cols\nnum_cols = df.select_dtypes(include='number').columns\n#print(num_cols)\n\nskewness_before = df[num_cols].skew().sort_values(ascending=False)\n#print(\"Before:\", skewness_before)\n\n# yeo-johnson power transformer\nyeojt = PowerTransformer(method='yeo-johnson', standardize=True)\ndf_transformed = yeojt.fit_transform(df[num_cols])\n\n# Create DataFrame from transformed data with correct columns\ndf_t = pd.DataFrame(df_transformed, columns=num_cols)\n\n# Overwrite original columns in df with transformed values\ndf[num_cols] = df_t\n\n# Get skewness after transformation\nskewness_after = df[num_cols].skew().sort_values(ascending=False)\n#print(\"After:\", skewness_after)\n\n# Combined before/after skewness chart\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Before transformation\nsns.barplot(x=skewness_before.values, y=skewness_before.index, ax=ax1, color='lightcoral')\nax1.set_title(\"Skewness Before Yeo-Johnson\", fontsize=14)\nax1.axvline(x=0, color='red', linestyle='--', alpha=0.7)\nax1.set_xlabel(\"Skewness\", fontsize=12)\n\n# After transformation\nsns.barplot(x=skewness_after.values, y=skewness_after.index, ax=ax2, color='lightblue')\nax2.set_title(\"Skewness After Yeo-Johnson\", fontsize=14)\nax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\nax2.set_xlabel(\"Skewness\", fontsize=12)\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n# encode target variable\nle = LabelEncoder()\ndf['encoded_emotion'] = le.fit_transform(df['emotion'])\n\n# View mapping\n#print(dict(zip(le.classes_, le.transform(le.classes_))))\n\n\n\n\n\n\n\n\n\n\nSplitting the dataset\nThe data is split using the train_test_split function, with 20% of the data reserved for testing. For the features (X), we drop the target column and its encoded variant, while for the labels (y), we retain only the encoded emotion.\n\n# train/test split for target emotions\nX = df.drop(columns=['emotion','encoded_emotion'])\ny = df['encoded_emotion']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n\n\nData Scaling and Dimension Reduction\nBefore training our models, we applied data scaling and dimensionality reduction to the numeric features. First, we identified all numeric columns in the dataset and applied Standard Scaling to ensure each feature has zero mean and unit variance.\nNext, we performed Principal Component Analysis (PCA) to reduce dimensionality while retaining 98% of the variance. PCA transforms the scaled features into a set of orthogonal components, capturing most of the information in fewer dimensions. We evaluated the explained variance ratio for each component and visualized it using a scree plot to confirm the number of components selected.\n\n# select all numeric columns\nnum_cols = X_train.select_dtypes(include='number').columns\n\n# Apply standard scaler on numeric data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train[num_cols])\nX_test_scaled = scaler.transform(X_test[num_cols])\n\n# Apply PCA while retaining 90\npca = PCA(n_components=.98)\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n\n# Explained variance ratio\nexplained_variance = pca.explained_variance_ratio_\n\n# Create scree plot\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(explained_variance)+1), explained_variance, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained')\nplt.xticks(range(1, len(explained_variance)+1))\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "index.html#model-training-and-evaluation",
    "href": "index.html#model-training-and-evaluation",
    "title": "Acoustic Emotion Classification",
    "section": "Model Training and Evaluation",
    "text": "Model Training and Evaluation\nTo address our research questions on emotion recognition from acoustic features, we trained and evaluated multiple machine learning models, including standalone algorithms, ensemble methods, and neural networks. The models were trained on the PCA-reduced and standardized feature set to improve convergence, reduce dimensionality, and mitigate potential overfitting.\nFor each model, we used the evaluate_model function, which trains the model and reports comprehensive performance metrics. These metrics include overall accuracy, macro-averaged precision, recall, and F1-score, as well as per-class performance for each emotion in the target set. To provide a visual assessment of prediction quality, confusion matrices were generated for all models.\nSpecifically, we evaluated: - Random Forest (RF): An ensemble method configured with 1000 trees, maximum depth of 15, and out-of-bag scoring to provide robust predictions while controlling for overfitting.\n\nSupport Vector Machine (SVM): A kernel-based model with RBF kernel, class-weight balancing for mild class imbalance, and probability estimates enabled.\nMultilayer Perceptron (MLP): A neural network with three hidden layers (128, 64, 32 neurons), early stopping, and a 10% validation split to monitor convergence.\n\n\n# Skip extending y_train - we want to exclude omitted emotions from predictions\n# Use original y_train that only contains target emotions\n\n# Get emotion labels for reporting\nemotion_labels = [emotion for emotion in le.classes_ if emotion in target_emotions]\ntarget_label_mapping = {le.transform([emotion])[0]: emotion for emotion in emotion_labels}\n\nprint(\"Target emotion label mapping:\", target_label_mapping)\n\ndef evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n    \"\"\"\n    Train and evaluate a model with comprehensive metrics\n    \"\"\"\n    print(f\"Training {model_name}\")\n    print(f\"{'='*50}\")\n    \n    # Train model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Overall metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n    \n    print(f\"\\n{model_name} Overall Performance:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Macro Precision: {precision_macro:.4f}\")\n    print(f\"Macro Recall: {recall_macro:.4f}\")\n    print(f\"Macro F1-Score: {f1_macro:.4f}\")\n    \n    # Per-class metrics\n    print(f\"\\n{model_name} Per-Class Performance:\")\n    target_names = [target_label_mapping.get(i, f'Class_{i}') for i in sorted(np.unique(y_test))]\n    \n    report = classification_report(y_test, y_pred, \n                                 target_names=target_names,\n                                 output_dict=True, \n                                 zero_division=0)\n    \n    # Display per-class metrics in a formatted way\n    print(f\"{'Emotion':&lt;10} {'Precision':&lt;10} {'Recall':&lt;10} {'F1-Score':&lt;10} {'Support':&lt;10}\")\n    print(\"-\" * 50)\n    \n    for emotion in target_names:\n        if emotion in report:\n            metrics = report[emotion]\n            print(f\"{emotion:&lt;10} {metrics['precision']:&lt;10.4f} {metrics['recall']:&lt;10.4f} \"\n                  f\"{metrics['f1-score']:&lt;10.4f} {int(metrics['support']):&lt;10}\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=target_names, yticklabels=target_names)\n    plt.title(f'{model_name} - Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # Return results for comparison\n    return {\n        'model_name': model_name,\n        'accuracy': accuracy,\n        'precision_macro': precision_macro,\n        'recall_macro': recall_macro,\n        'f1_macro': f1_macro,\n        'per_class_report': report\n    }\n\n# Initialize models\n# random forest\nrf_model = RandomForestClassifier(\n    n_estimators=1000,          # More trees for better performance\n    max_depth=15,               # Prevent overfitting while allowing complexity\n    min_samples_split=5,        # Require more samples to split\n    min_samples_leaf=2,         # Minimum samples in leaf nodes\n    max_features='sqrt',        # Feature sampling at each split\n    bootstrap=True,             # Bootstrap sampling\n    oob_score=True,             # Out-of-bag scoring\n    n_jobs=-1,                  # Use all available cores\n    random_state=42\n)\n# support vector machine\nsvm_model = SVC(\n    kernel='rbf',               # RBF kernel works well for most cases\n    C=10.0,                     # Regularization parameter\n    gamma='scale',              # Kernel coefficient (auto-scaled)\n    probability=True,           # Enable probability estimates\n    cache_size=1000,            # Increase cache for faster training\n    class_weight='balanced',    # Handle imbalanced datasets\n    random_state=42\n)\n\n# standard neral network \nmlp_model = MLPClassifier(\n    hidden_layer_sizes=(128, 64, 32), # network\n    max_iter=1000,                    # Maximum iterations                                \n    early_stopping=True,              # Stop when validation score stops improving\n    validation_fraction=0.1,          # validation set\n    random_state=42\n)\n\n# Evaluate ensemble models along with stand alone models\nresults = []\n\n# Random Forest\nrf_results = evaluate_model(rf_model, X_train_pca, X_test_pca, \n                           y_train, y_test, \"Random Forest\")\nresults.append(rf_results)\n\n# SVM\nsvm_results = evaluate_model(svm_model, X_train_pca, X_test_pca, \n                            y_train, y_test, \"SVM\")\nresults.append(svm_results)\n\n# MLP\nmlp_results = evaluate_model(mlp_model, X_train_pca, X_test_pca, \n                            y_train, y_test, \"MLP(Neural Net)\")\nresults.append(mlp_results)\n\nTarget emotion label mapping: {np.int64(0): 'angry', np.int64(1): 'disgust', np.int64(2): 'fear', np.int64(3): 'happy', np.int64(4): 'neutral', np.int64(5): 'sad'}\nTraining Random Forest\n==================================================\n\nRandom Forest Overall Performance:\nAccuracy: 0.5124\nMacro Precision: 0.5045\nMacro Recall: 0.5121\nMacro F1-Score: 0.5014\n\nRandom Forest Per-Class Performance:\nEmotion    Precision  Recall     F1-Score   Support   \n--------------------------------------------------\nangry      0.6067     0.7835     0.6838     254       \ndisgust    0.4744     0.4370     0.4549     254       \nfear       0.4834     0.2874     0.3605     254       \nhappy      0.4938     0.4667     0.4798     255       \nneutral    0.4519     0.4954     0.4726     218       \nsad        0.5169     0.6024     0.5564     254       \n\n\n\n\n\n\n\n\n\nTraining SVM\n==================================================\n\nSVM Overall Performance:\nAccuracy: 0.5285\nMacro Precision: 0.5258\nMacro Recall: 0.5280\nMacro F1-Score: 0.5256\n\nSVM Per-Class Performance:\nEmotion    Precision  Recall     F1-Score   Support   \n--------------------------------------------------\nangry      0.6541     0.7520     0.6996     254       \ndisgust    0.4960     0.4921     0.4941     254       \nfear       0.4478     0.4724     0.4598     254       \nhappy      0.5152     0.4667     0.4897     255       \nneutral    0.4846     0.5046     0.4944     218       \nsad        0.5571     0.4803     0.5159     254       \n\n\n\n\n\n\n\n\n\nTraining MLP(Neural Net)\n==================================================\n\nMLP(Neural Net) Overall Performance:\nAccuracy: 0.5567\nMacro Precision: 0.5538\nMacro Recall: 0.5574\nMacro F1-Score: 0.5534\n\nMLP(Neural Net) Per-Class Performance:\nEmotion    Precision  Recall     F1-Score   Support   \n--------------------------------------------------\nangry      0.6996     0.7795     0.7374     254       \ndisgust    0.5205     0.4488     0.4820     254       \nfear       0.4845     0.4921     0.4883     254       \nhappy      0.5463     0.4627     0.5011     255       \nneutral    0.4792     0.5826     0.5259     218       \nsad        0.5927     0.5787     0.5857     254"
  },
  {
    "objectID": "index.html#model-comparision",
    "href": "index.html#model-comparision",
    "title": "Acoustic Emotion Classification",
    "section": "Model Comparision",
    "text": "Model Comparision\nWe compared the performance of three model types—Random Forest, SVM, and a multilayer perceptron (MLP), on the emotion recognition task using macro-averaged metrics and per-emotion performance. Across overall performance metrics, the MLP consistently outperformed both Random Forest and SVM, achieving the highest accuracy (0.5567), macro precision (0.5538), macro recall (0.5574), and macro F1-score (0.5534). This indicates that the neural network is most effective at capturing complex patterns in the PCA-reduced acoustic feature space.\nPer-emotion analysis revealed nuanced differences among models. For Angry, all models performed relatively well, with MLP achieving the highest F1-score (0.7374). Disgust and Fear were more challenging emotions, with lower F1-scores overall, though MLP slightly improved performance over other models. For Happy and Neutral, MLP again showed superior F1-scores, particularly in improving recall for the Neutral class (0.5826). Sad emotion classification also favored MLP, demonstrating balanced precision and recall (F1-score 0.5857).\nOverall, while ensemble methods like Random Forest and kernel-based SVM provide competitive performance for certain classes, the MLP’s ability to model non-linear interactions across multiple dimensions makes it the best-performing approach in this task. These results highlight that neural network-based models may be better suited for emotion recognition from acoustic features, addressing both classification accuracy and balanced performance across all emotions.\n\nprint(\"MODEL COMPARISON SUMMARY\")\nprint(f\"{'='*50}\")\n\n# Create comparison DataFrame\ncomparison_df = pd.DataFrame({\n    'Model': [result['model_name'] for result in results],\n    'Accuracy': [result['accuracy'] for result in results],\n    'Precision (Macro)': [result['precision_macro'] for result in results],\n    'Recall (Macro)': [result['recall_macro'] for result in results],\n    'F1-Score (Macro)': [result['f1_macro'] for result in results]\n})\n\nprint(comparison_df.round(4))\n\n# Visualize model comparison\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\nmetrics = ['Accuracy', 'Precision (Macro)', 'Recall (Macro)', 'F1-Score (Macro)']\ncolors = ['skyblue', 'lightcoral', 'lightgreen']\n\nfor i, metric in enumerate(metrics):\n    row = i // 2\n    col = i % 2\n    \n    ax = axes[row, col]\n    bars = ax.bar(comparison_df['Model'], comparison_df[metric], color=colors)\n    ax.set_title(f'{metric} Comparison')\n    ax.set_ylabel(metric)\n    ax.set_ylim(0, 1)\n    \n    # Add value labels on bars\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                f'{height:.3f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# Best performing model\nbest_model_idx = comparison_df['F1-Score (Macro)'].idxmax()\nbest_model = comparison_df.iloc[best_model_idx]\n\nprint(f\"\\nBest Performing Model: {best_model['Model']}\")\nprint(f\"F1-Score (Macro): {best_model['F1-Score (Macro)']:.4f}\")\n\n# Detailed per-emotion analysis across all models\nprint(\"PER-EMOTION PERFORMANCE ACROSS MODELS\")\nprint(f\"{'='*50}\")\n\n# Get target emotion names from the unique test labels\ntarget_names = [target_label_mapping.get(i, f'Class_{i}') for i in sorted(np.unique(y_test))]\n\nemotion_performance = {}\nfor emotion in target_names:\n    emotion_performance[emotion] = {}\n    for result in results:\n        if emotion in result['per_class_report']:\n            emotion_performance[emotion][result['model_name']] = {\n                'precision': result['per_class_report'][emotion]['precision'],\n                'recall': result['per_class_report'][emotion]['recall'],\n                'f1_score': result['per_class_report'][emotion]['f1-score']\n            }\n\n# Create detailed emotion analysis\nfor emotion in target_names:\n    print(f\"\\n{emotion.upper()} Performance:\")\n    print(f\"{'Model':&lt;15} {'Precision':&lt;10} {'Recall':&lt;10} {'F1-Score':&lt;10}\")\n    print(\"-\" * 50)\n    \n    for model_name, metrics in emotion_performance[emotion].items():\n        print(f\"{model_name:&lt;15} {metrics['precision']:&lt;10.4f} {metrics['recall']:&lt;10.4f} {metrics['f1_score']:&lt;10.4f}\")\n\nMODEL COMPARISON SUMMARY\n==================================================\n             Model  Accuracy  Precision (Macro)  Recall (Macro)  \\\n0    Random Forest    0.5124             0.5045          0.5121   \n1              SVM    0.5285             0.5258          0.5280   \n2  MLP(Neural Net)    0.5567             0.5538          0.5574   \n\n   F1-Score (Macro)  \n0            0.5014  \n1            0.5256  \n2            0.5534  \n\n\n\n\n\n\n\n\n\n\nBest Performing Model: MLP(Neural Net)\nF1-Score (Macro): 0.5534\nPER-EMOTION PERFORMANCE ACROSS MODELS\n==================================================\n\nANGRY Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.6067     0.7835     0.6838    \nSVM             0.6541     0.7520     0.6996    \nMLP(Neural Net) 0.6996     0.7795     0.7374    \n\nDISGUST Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.4744     0.4370     0.4549    \nSVM             0.4960     0.4921     0.4941    \nMLP(Neural Net) 0.5205     0.4488     0.4820    \n\nFEAR Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.4834     0.2874     0.3605    \nSVM             0.4478     0.4724     0.4598    \nMLP(Neural Net) 0.4845     0.4921     0.4883    \n\nHAPPY Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.4938     0.4667     0.4798    \nSVM             0.5152     0.4667     0.4897    \nMLP(Neural Net) 0.5463     0.4627     0.5011    \n\nNEUTRAL Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.4519     0.4954     0.4726    \nSVM             0.4846     0.5046     0.4944    \nMLP(Neural Net) 0.4792     0.5826     0.5259    \n\nSAD Performance:\nModel           Precision  Recall     F1-Score  \n--------------------------------------------------\nRandom Forest   0.5169     0.6024     0.5564    \nSVM             0.5571     0.4803     0.5159    \nMLP(Neural Net) 0.5927     0.5787     0.5857"
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Acoustic Emotion Classification",
    "section": "Conclusion",
    "text": "Conclusion\nThis study investigated the capability of machine learning models to classify emotional states from acoustic features using the CREMA-D dataset. Across multiple approaches including Random Forest, SVM, and a multilayer perceptron, the MLP consistently demonstrated superior performance in both overall accuracy and per-emotion metrics, highlighting its ability to capture complex nonlinear patterns in the PCA-reduced feature space. While traditional ensemble and kernel based methods provided competitive results for certain emotions, neural networks offered the most balanced performance across all classes, particularly for challenging emotions such as Disgust, Fear, and Neutral.\nThese findings suggest that for multi-class emotion recognition from audio, neural networks are better suited to leverage nuanced acoustic features compared to standalone or ensemble methods. Future work could explore integrating temporal modeling with recurrent neural networks or transformer based architectures, combining audio and visual modalities, or experimenting with advanced feature extraction methods to further improve classification accuracy and robustness."
  },
  {
    "objectID": "presentation.html#abstract",
    "href": "presentation.html#abstract",
    "title": "Acoustic Emotion Classification",
    "section": "Abstract",
    "text": "Abstract\n\nDataset: CREMA-D, six emotions (neutral, happy, sad, angry, fear, disgust)\nFeatures: librosa, standardized, PCA (98% variance)\nModels: SVM, Random Forest, MLP\nMLP achieved macro F1-score: 0.5534"
  },
  {
    "objectID": "presentation.html#introduction",
    "href": "presentation.html#introduction",
    "title": "Acoustic Emotion Classification",
    "section": "Introduction",
    "text": "Introduction\n\nAutomatic emotion classification is challenging\nCREMA-D: 7,442 clips, 91 actors\nFeatures: MFCCs, spectral properties\nGoal: Compare traditional, ensemble, and neural networks"
  },
  {
    "objectID": "presentation.html#research-question",
    "href": "presentation.html#research-question",
    "title": "Acoustic Emotion Classification",
    "section": "Research Question",
    "text": "Research Question\n\nQ1. What is the classification accuracy of unsupervised methods for emotion recognition from acoustic features?\nQ2. How do standalone algorithms, ensemble approaches, and neural networks compare in terms of accuracy, robustness, and computational efficiency for emotion classification from audio?"
  },
  {
    "objectID": "presentation.html#eda",
    "href": "presentation.html#eda",
    "title": "Acoustic Emotion Classification",
    "section": "EDA",
    "text": "EDA\n\nData OverviewTarget Distribution\n\n\n\n\nCode\n# number of variables and observations in the data\nprint(f\"Total observations: {df.shape[0]}\")\nprint(f\"Number of features: {df.shape[1]}\")\n\n# missing values in each column\nmissing_df = df.isna().sum()\nprint(\"Numeric Description:\\n\", df.describe())\n\n\nTotal observations: 7442\nNumber of features: 41\nNumeric Description:\n           actor_id  audio_duration  sample_rate  mfcc_1_mean   mfcc_1_std  \\\ncount  7442.000000     7442.000000       7442.0  7442.000000  7442.000000   \nmean   1046.084117        2.542910      22050.0  -387.893237    81.152242   \nstd      26.243152        0.505980          0.0    56.912883    30.241790   \nmin    1001.000000        1.267982      22050.0 -1131.370700     0.000122   \n25%    1023.000000        2.202222      22050.0  -428.004615    58.292865   \n50%    1046.000000        2.502540      22050.0  -399.767640    76.243485   \n75%    1069.000000        2.836190      22050.0  -354.018697   101.771385   \nmax    1091.000000        5.005034      22050.0  -162.543350   179.528880   \n\n       mfcc_2_mean   mfcc_2_std  mfcc_3_mean   mfcc_3_std  mfcc_4_mean  ...  \\\ncount  7442.000000  7442.000000  7442.000000  7442.000000  7442.000000  ...   \nmean    131.246557    26.349166     7.226425    31.621393    50.164769  ...   \nstd      15.557340     6.193413    11.605281    11.700738    11.128262  ...   \nmin       0.000000     0.000000   -52.340374     0.000000     0.000000  ...   \n25%     122.297443    22.112123     0.674029    22.746978    42.658050  ...   \n50%     134.065410    25.892035     8.938592    30.019723    50.710929  ...   \n75%     142.583675    30.226819    15.246822    39.455707    58.216644  ...   \nmax     167.168330    63.146930    38.951794    73.551254    83.296500  ...   \n\n       mfcc_13_std  spectral_centroid_mean  spectral_centroid_std  \\\ncount  7442.000000             7442.000000            7442.000000   \nmean      6.486291             1391.389433             569.861009   \nstd       1.678969              254.030203             284.309588   \nmin       0.000000                0.000000               0.000000   \n25%       5.387114             1213.176905             356.601232   \n50%       6.221665             1335.982229             507.373775   \n75%       7.244229             1510.743971             725.217080   \nmax      24.734776             2873.927831            1699.906329   \n\n       spectral_rolloff_mean  spectral_bandwidth_mean     rms_mean  \\\ncount            7442.000000              7442.000000  7442.000000   \nmean             2959.971329              1748.984424     0.027548   \nstd               471.242990               115.947135     0.028312   \nmin                 0.000000                 0.000000     0.000000   \n25%              2648.811001              1679.011486     0.010933   \n50%              2926.667949              1748.878905     0.016707   \n75%              3211.563802              1815.428594     0.032007   \nmax              5258.158543              2163.024688     0.223023   \n\n           rms_std     zcr_mean  chroma_mean   chroma_std  \ncount  7442.000000  7442.000000  7442.000000  7442.000000  \nmean      0.027249     0.063343     0.389487     0.301066  \nstd       0.031667     0.023750     0.045327     0.012338  \nmin       0.000000     0.000000     0.000000     0.000000  \n25%       0.008040     0.047160     0.359097     0.293473  \n50%       0.015029     0.056566     0.389878     0.301533  \n75%       0.033126     0.072023     0.420108     0.309572  \nmax       0.220164     0.233774     0.553840     0.335121  \n\n[8 rows x 38 columns]\n\n\n\n\n\n\n\nTarget Class Count Plot"
  },
  {
    "objectID": "presentation.html#data-preprocessing",
    "href": "presentation.html#data-preprocessing",
    "title": "Acoustic Emotion Classification",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nData PreparationSkewness Transformation\n\n\n\nRemove irrelevant columns, handle missing values\nApply Yeo-Johnson Power Transformation for numeric skew\nEncode target labels\n\n\n\n\n\n\nSknewness Before & After Yeo Johnson Transformation"
  },
  {
    "objectID": "presentation.html#feature-engineering",
    "href": "presentation.html#feature-engineering",
    "title": "Acoustic Emotion Classification",
    "section": "Feature engineering",
    "text": "Feature engineering\n\n\nSpectral Contrast: Measures amplitude differences between spectral peaks and valleys, capturing timbral characteristics that distinguish emotional expressions\nMFCCs (Mel-frequency cepstral coefficients): Extract 13 coefficients representing the short-term power spectrum, fundamental for speech emotion recognition\nChroma Features: Capture pitch class energy distribution, providing harmonic content information relevant to emotional prosody\nZero-Crossing Rate: Quantifies signal noisiness by measuring zero-axis crossings, distinguishing between voiced and unvoiced speech segments\nRoot Mean Square (RMS) Energy: Measures overall signal energy, correlating with loudness and emotional intensity"
  },
  {
    "objectID": "presentation.html#principal-component-analysis",
    "href": "presentation.html#principal-component-analysis",
    "title": "Acoustic Emotion Classification",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nPCA: retain 98% variance"
  },
  {
    "objectID": "presentation.html#model-evaulation-function",
    "href": "presentation.html#model-evaulation-function",
    "title": "Acoustic Emotion Classification",
    "section": "Model Evaulation Function",
    "text": "Model Evaulation Function\n\n\n\n\nInputs:\n\nmodel → ML model instance (Random Forest, SVM, MLP)\nX_train, X_test → feature matrices\ny_train, y_test → labels\nmodel_name → string for labeling outputs\n\n\n\n\nOutput:\n\nConsole print of metrics & confusion matrix\nDictionary with overall and per-class performance"
  },
  {
    "objectID": "presentation.html#model-performance",
    "href": "presentation.html#model-performance",
    "title": "Acoustic Emotion Classification",
    "section": "Model Performance",
    "text": "Model Performance\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nRF\n0.52\n0.51\n0.515\n0.512\n\n\nSVM\n0.49\n0.48\n0.487\n0.482\n\n\nMLP\n0.557\n0.554\n0.557\n0.553"
  },
  {
    "objectID": "presentation.html#model-metric-comparison",
    "href": "presentation.html#model-metric-comparison",
    "title": "Acoustic Emotion Classification",
    "section": "Model Metric Comparison",
    "text": "Model Metric Comparison\n\nModel Metric (Accuracy, Precision, Recall, F1-Score) Comparison"
  },
  {
    "objectID": "presentation.html#random-forest-predictions",
    "href": "presentation.html#random-forest-predictions",
    "title": "Acoustic Emotion Classification",
    "section": "Random Forest Predictions",
    "text": "Random Forest Predictions\n\nRandom Forest; Confusion Matrix"
  },
  {
    "objectID": "presentation.html#support-vector-machine-predictions",
    "href": "presentation.html#support-vector-machine-predictions",
    "title": "Acoustic Emotion Classification",
    "section": "Support Vector Machine Predictions",
    "text": "Support Vector Machine Predictions\n\nSupport Vector Machine; Confusion Matrix"
  },
  {
    "objectID": "presentation.html#multi-layer-perceptron-predictions",
    "href": "presentation.html#multi-layer-perceptron-predictions",
    "title": "Acoustic Emotion Classification",
    "section": "Multi Layer Perceptron Predictions",
    "text": "Multi Layer Perceptron Predictions\n\nMulti Layer Perceptron; Confusion Matrix"
  },
  {
    "objectID": "presentation.html#summary",
    "href": "presentation.html#summary",
    "title": "Acoustic Emotion Classification",
    "section": "Summary",
    "text": "Summary\n\nMLP is the best-performing model for multi-class emotion recognition\nNeural networks capture complex, non-linear patterns better than traditional or ensemble methods\nFuture Work: temporal modeling, multimodal integration, advanced feature extraction"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acoustic Emotion Classification: Optimizing Performance Through Ensemble Methods",
    "section": "",
    "text": "This project was developed by Ralph Andrade For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nRalph Andrade: 2027, MS Data Science"
  }
]